[
  {
    "objectID": "slides/week1.html#r-in-the-terminal",
    "href": "slides/week1.html#r-in-the-terminal",
    "title": "Welcome!",
    "section": "R in the terminal",
    "text": "R in the terminal"
  },
  {
    "objectID": "slides/week1.html#r-rstudio",
    "href": "slides/week1.html#r-rstudio",
    "title": "Welcome!",
    "section": "R + RStudio",
    "text": "R + RStudio"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intro to PHDS",
    "section": "",
    "text": "Term: Fall 2024\nTime: Friday 9am - 12:55pm\nLocation: SSB 114\nGitHub"
  },
  {
    "objectID": "index.html#pm-566-introduction-to-health-data-science",
    "href": "index.html#pm-566-introduction-to-health-data-science",
    "title": "Intro to PHDS",
    "section": "",
    "text": "Term: Fall 2024\nTime: Friday 9am - 12:55pm\nLocation: SSB 114\nGitHub"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Term: Fall 2024\nTime: Friday 9am - 12:55pm\nLocation: SSB 114\nUnits: 4"
  },
  {
    "objectID": "syllabus.html#pm-566-introduction-to-health-data-science",
    "href": "syllabus.html#pm-566-introduction-to-health-data-science",
    "title": "Syllabus",
    "section": "",
    "text": "Term: Fall 2024\nTime: Friday 9am - 12:55pm\nLocation: SSB 114\nUnits: 4"
  },
  {
    "objectID": "syllabus.html#course-overview",
    "href": "syllabus.html#course-overview",
    "title": "Syllabus",
    "section": "Course Overview",
    "text": "Course Overview\nThis course serves as an introduction to data science with a focus on the acquisition and analysis of real-life data. Students will learn the tools needed to: 1. Create usable and reproducible datasets by accessing, scraping, and cleaning data 2. Conduct exploratory data analysis and visualization 3. Identify scientific questions that can be answered with a given dataset 4. Write functional code in the R programming language, build basic apps, and construct a website"
  },
  {
    "objectID": "syllabus.html#learning-objectives",
    "href": "syllabus.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThrough this course, students will become familiar with the techniques used in data science and apply them to health-related datasets. Students will learn:\n\nProgramming in R and associated tools including Quarto, Git, and SQL\nData visualization – selecting appropriate plots to gain insight from data\nData collection – web scraping, data wrangling, and database management\nExploratory data analysis – generating hypotheses while building intuition and understanding of a dataset\nBasic computational algorithms, including simulation strategies\nBuilding interactive tools and websites\n\nPrerequisite(s): None\nRecommended Preparation: Familiarity with programming, particularly in the R language"
  },
  {
    "objectID": "syllabus.html#course-notes",
    "href": "syllabus.html#course-notes",
    "title": "Syllabus",
    "section": "Course Notes",
    "text": "Course Notes\nLecture notes presented in class will be posted on GitHub."
  },
  {
    "objectID": "syllabus.html#technological-proficiency-and-hardwaresoftware-required",
    "href": "syllabus.html#technological-proficiency-and-hardwaresoftware-required",
    "title": "Syllabus",
    "section": "Technological Proficiency and Hardware/Software Required",
    "text": "Technological Proficiency and Hardware/Software Required\nThe R language (http://cran.r-project.org) will be used throughout the semester and we recommend using the R Studio IDE for coding (https://posit.co/download/rstudio-desktop/). Additionally, if they do not already have one, students will be required to create a GitHub account (https://github.com/)."
  },
  {
    "objectID": "syllabus.html#readings-and-supplementary-materials",
    "href": "syllabus.html#readings-and-supplementary-materials",
    "title": "Syllabus",
    "section": "Readings and Supplementary Materials",
    "text": "Readings and Supplementary Materials\nThere are no required readings for this course."
  },
  {
    "objectID": "syllabus.html#supplementary-references",
    "href": "syllabus.html#supplementary-references",
    "title": "Syllabus",
    "section": "Supplementary References",
    "text": "Supplementary References\n\n\nR Programming for Data Science, 2019. Roger Peng. https://bookdown.org/rdpeng/rprogdatascience/\n\nR for Data Science, 2017. Garrett Grolemund and Hadley Wickham. http://r4ds.had.co.nz/\nExploratory Data Analysis with R, 2020. Roger Peng. https://bookdown.org/rdpeng/exdata/\nMastering Software Development in R, 2017. Roger Peng, Sean Kross, Brooke Anderson. https://bookdown.org/rdpeng/RProgDA/\nR Packages, 2023. Hadley Wickham and Jennifer Bryan. https://r-pkgs.org/\nModern Data Science with R, 2023. Benjamin S. Baumer, Daniel T. Kaplan, and Nicholas J. Horton. https://mdsr-book.github.io/mdsr3e/"
  },
  {
    "objectID": "syllabus.html#description-and-assessment-of-assignments",
    "href": "syllabus.html#description-and-assessment-of-assignments",
    "title": "Syllabus",
    "section": "Description and Assessment of Assignments",
    "text": "Description and Assessment of Assignments\nAssignments: There will be 5 assignments given throughout the semester, approximately 1 every 2 weeks. Students may discuss the problems with one another, however, individual solutions must be submitted and copying will not be tolerated. All assignments must be completed in Quarto or R Markdown, and submitted through the Github classes portal of the course. Late assignments will be penalized by 20% for each day past the due date.\nFinal Project: The final project will be to write a report for an analysis applied to a real-world dataset and to create a website that includes interactive visualizations to display the data and results. The source code, website files, and PDF report will be uploaded to GitHub.\nLabs: There will be weekly lab assignments which are graded for completion. Each week, there will be class time devoted to working on that week’s lab. Completing the weekly labs will count as part of the overall grade."
  },
  {
    "objectID": "syllabus.html#grading-breakdown",
    "href": "syllabus.html#grading-breakdown",
    "title": "Syllabus",
    "section": "Grading Breakdown",
    "text": "Grading Breakdown\n\n\n\nAssignment\n% of Grade\n\n\n\n\nLabs\n20%\n\n\nHomework (5)\n30%\n\n\nMidterm Exam\n20%\n\n\nFinal Project\n30%\n\n\nTOTAL\n100%"
  },
  {
    "objectID": "syllabus.html#assignment-submission-policy",
    "href": "syllabus.html#assignment-submission-policy",
    "title": "Syllabus",
    "section": "Assignment Submission Policy",
    "text": "Assignment Submission Policy\n/////////////////////////////////////////// Assignments shall be submitted on the Github classroom portal of the course. Late homework assignments will not be accepted without penalty, except when verifiable extenuating circumstances can be demonstrated."
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "Syllabus",
    "section": "Schedule",
    "text": "Schedule\nAs the weeks go by, consult the Schedule Page for more information on weekly topics, problem sets, readings, and other materials. The schedule is likely to change as we go. Links to readings, assignments, and other materials from class will be posted on that page."
  },
  {
    "objectID": "syllabus.html#academic-integrity",
    "href": "syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nThe University of Southern California is foremost a learning community committed to fostering successful scholars and researchers dedicated to the pursuit of knowledge and the transmission of ideas. Academic misconduct is in contrast to the university’s mission to educate students through a broad array of first-rank academic, professional, and extracurricular programs and includes any act of dishonesty in the submission of academic work (either in draft or final form).\nThis course will follow the expectations for academic integrity as stated in the USC Student Handbook. All students are expected to submit assignments that are original work and prepared specifically for the course/section in this academic term. You may not submit work written by others or “recycle” work prepared for other courses without obtaining written permission from the instructor(s). Students suspected of engaging in academic misconduct will be reported to the Office of Academic Integrity.\nOther violations of academic misconduct include, but are not limited to, cheating, plagiarism, fabrication (e.g., falsifying data), knowingly assisting others in acts of academic dishonesty, and any act that gains or is intended to gain an unfair academic advantage.\nThe impact of academic dishonesty is far-reaching and is considered a serious offense against the university and could result in outcomes such as failure on the assignment, failure in the course, suspension, or even expulsion from the university.\nFor more information about academic integrity see the student handbook or the Office of Academic Integrity’s website, and university policies on Research and Scholarship Misconduct.\n\nStatement on the use of Artificial Intelligence\nGenerative artificial intelligence (AI) may be used under the direction and rules specified by the course instructor in specific circumstances as outlined in the syllabus. The student is responsible for the quality and content of all written assignments. Unless otherwise indicated by the course instructor, generative AI may be used to create an initial literature review, document outline and/or to organize material toward a first draft of a class paper, proofreading, or grammatical accuracy; however the final content of the written document and critical thinking of the ideas presented in the document must represent the student’s individual work and ideas learned through course content and/or research conducted from sources outside of the generative AI system. The student must include an annotation on all materials submitted that explicitly documents how AI was used to generate the document and properly reference both the sources and the AI tools such as ChatGPT (OpenAI, 2023). The student must review the information in the document and edit for accuracy, completeness, proper grammar, and demonstrate that the wording accurately reflects the student’s understanding and purpose in writing the text. Students should be aware that text generated solely from AI generators may include factual errors, bias, and may contain incomplete or inaccurate reference information, in addition to furthering appropriating knowledge produced by historically marginalized scholars without proper crediting. If you have any questions on whether a specific AI tool is allowed for any aspect of your work in this class, please ask your instructor for guidance. Failure to ensure agreement with your instructor on use of AI, prior to doing so, may result in a zero score. (NOTE: instructors have sophisticated tools to determine AI plagiarism.)"
  },
  {
    "objectID": "syllabus.html#students-and-disability-accommodations",
    "href": "syllabus.html#students-and-disability-accommodations",
    "title": "Syllabus",
    "section": "Students and Disability Accommodations:",
    "text": "Students and Disability Accommodations:\nUSC welcomes students with disabilities into all of the University’s educational programs. The Office of Student Accessibility Services (OSAS) is responsible for the determination of appropriate accommodations for students who encounter disability-related barriers. Once a student has completed the OSAS process (registration, initial appointment, and submitted documentation) and accommodations are determined to be reasonable and appropriate, a Letter of Accommodation (LOA) will be available to generate for each course. The LOA must be given to each course instructor by the student and followed up with a discussion. This should be done as early in the semester as possible as accommodations are not retroactive. More information can be found at http://osas.usc.edu. You may contact OSAS at (213) 740-0776 or via email at osasfrontdesk@usc.edu."
  },
  {
    "objectID": "syllabus.html#support-systems",
    "href": "syllabus.html#support-systems",
    "title": "Syllabus",
    "section": "Support Systems:",
    "text": "Support Systems:\nCounseling and Mental Health - (213) 740-9355 – 24/7 on call\nhttps://studenthealth.usc.edu/counseling/\nFree and confidential mental health treatment for students, including short-term psychotherapy, group counseling, stress fitness workshops, and crisis intervention.\nNational Suicide Prevention Lifeline - dial 988 – 24/7 on call\nhttp://www.suicidepreventionlifeline.org\nProvides free and confidential emotional support to people in suicidal crisis or emotional distress 24 hours a day, 7 days a week.\nRelationship and Sexual Violence Prevention Services (RSVP) - (213) 740-9355(WELL), press “0” after hours – 24/7 on call\nhttps://studenthealth.usc.edu/sexual-assault\nFree and confidential therapy services, workshops, and training for situations related to gender-based harm.\nOffice for Equity, Equal Opportunity, and Title IX (EEO-TIX) - (213) 740-5086\nhttp://eeotix.usc.edu\nInformation about how to get help or help someone affected by harassment or discrimination, rights of protected classes, reporting options, and additional resources for students, faculty, staff, visitors, and applicants.\nReporting Incidents of Bias or Harassment - (213) 740-5086 or (213) 821-8298\nhttp://usc-advocate.symplicity.com/care_report\nAvenue to report incidents of bias, hate crimes, and microaggressions to the Office for Equity, Equal Opportunity, and Title for appropriate investigation, supportive measures, and response.\nThe Office of Student Accessibility Services (OSAS) - (213) 740-0776\nhttp://osas.usc.edu\nOSAS ensures equal access for students with disabilities through providing academic accommodations and auxiliary aids in accordance with federal laws and university policy.\nUSC Campus Support and Intervention - (213) 821-4710\nhttp://campussupport.usc.edu\nAssists students and families in resolving complex personal, financial, and academic issues adversely affecting their success as a student.\nDiversity, Equity and Inclusion - (213) 740-2101\nhttp://diversity.usc.edu\nInformation on events, programs and training, the Provost’s Diversity and Inclusion Council, Diversity Liaisons for each academic school, chronology, participation, and various resources for students.\nUSC Emergency - UPC: (213) 740-4321, HSC: (323) 442-1000 – 24/7 on call\nhttp://dps.usc.edu, http://emergency.usc.edu\nEmergency assistance and avenue to report a crime. Latest updates regarding safety, including ways in which instruction will be continued if an officially declared emergency makes travel to campus infeasible.\nUSC Department of Public Safety - UPC: (213) 740-6000, HSC: (323) 442-1200 – 24/7 on call\nhttp://dps.usc.edu\nNon-emergency assistance or information.\nOffice of the Ombuds - (213) 821-9556 (UPC) / (323-442-0382 (HSC)\nhttp://ombuds.usc.edu\nA safe and confidential place to share your USC-related issues with a University Ombuds who will work with you to explore options or paths to manage your concern.\nOccupational Therapy Faculty Practice - (323) 442-3340 or\notfp@med.usc.edu, http://chan.usc.edu/otfp\nConfidential Lifestyle Redesign services for USC students to support health promoting habits and routines that enhance quality of life and academic performance."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Week\nDate\nTopic\nSlides\nLab\nAssignment\n\n\n\n\n1\n8-23\nIntroduction to Data Science and R\nSlides\nLab\n\n\n\n2\n8-30\nVersion Control & Reproducible Research\nSlides\nLab\n\n\n\n3\n9-??\nExploratory data analysis\nSlides\nLab\n\n\n\n4\n9-??\nData visualization\nSlides\nLab\nHW1"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "R. A platform for statistical computing.\nRStudio. An IDE for R. The most straightforward way to get into using R and Quarto.\nR Graphics Cookbook. Complete guide to plotting data with ggplot.\nR Style Guide. Write readable code.\nRStudio Cheatsheets Other quick guides, including information about using RStudio’s IDE and some of the main tools in R."
  },
  {
    "objectID": "references.html#r",
    "href": "references.html#r",
    "title": "References",
    "section": "",
    "text": "R. A platform for statistical computing.\nRStudio. An IDE for R. The most straightforward way to get into using R and Quarto.\nR Graphics Cookbook. Complete guide to plotting data with ggplot.\nR Style Guide. Write readable code.\nRStudio Cheatsheets Other quick guides, including information about using RStudio’s IDE and some of the main tools in R."
  },
  {
    "objectID": "references.html#quarto",
    "href": "references.html#quarto",
    "title": "References",
    "section": "Quarto",
    "text": "Quarto\n\nQuarto An integrated, open-source publishing system. Generalizes and expands upon a lot of the functionality of RMarkdown.\nQuarto Guide Comprehensive guide to creating a wide range of documents and presentations with Quarto."
  },
  {
    "objectID": "references.html#git-github",
    "href": "references.html#git-github",
    "title": "References",
    "section": "Git / GitHub",
    "text": "Git / GitHub\n\nGit. Version control system. Installs with Apple’s Developer Tools, or get the latest version via Homebrew.\nGitHub. Host public Git repositories for free. Pay to host private ones. Also a source for publicly available code (e.g. R packages and utilities) written by other people.\nGitHub Docs Tutorials for performing various actions using git and GitHub, from beginner to advanced."
  },
  {
    "objectID": "references.html#markdown-r-markdown",
    "href": "references.html#markdown-r-markdown",
    "title": "References",
    "section": "Markdown / R Markdown",
    "text": "Markdown / R Markdown\n\nMarkdown tutorial: An interactive tutorial to practice using Markdown.\nMarkdown cheatsheet: Useful one-page reminder of Markdown syntax.\nR Markdown Cheatsheet An overview of Markdown and RMarkdown conventions.\nR Markdown documentation from the makers of RStudio. Lots of good examples."
  },
  {
    "objectID": "references.html#data-science",
    "href": "references.html#data-science",
    "title": "References",
    "section": "Data Science",
    "text": "Data Science\n\nViridical Data Science Great book on the practice of data science by Bin Yu and Rebecca Barter.\nR Programming for Data Science Book on using R for data science, by Roger Peng.\nJenny Bryan’s Stat 545. Notes and tutorials for a Data Analysis course taught by Jennifer Bryan at the University of British Columbia. Lots of useful material.\nThe Plain Person’s Guide to Plain Text Social Science: Why you should write data-based reports using plain-text tools.\nKarl Broman’s Tutorials and Guides Accurate and concise guides to many of the tools and topics described here, including getting started with reproducible research, using git and GitHub, and working with knitr.\nMakefiles for OCR and converting Shapefiles. Some further examples of Makefiles in the data-analysis pipeline, by Lincoln Mullen"
  },
  {
    "objectID": "references.html#tools",
    "href": "references.html#tools",
    "title": "References",
    "section": "Tools",
    "text": "Tools\n\nApple’s Developer Tools Unix toolchain. Install directly with xcode-select --install, or just try to use e.g. git from the terminal and have OS X prompt you to install the tools.\nHomebrew package manager. A convenient way to install several of the tools here, including Emacs and Pandoc.\nR. A platform for statistical computing.\nPython and SciPy. Python is a general-purpose programming language increasingly used in data manipulation and analysis.\nRStudio. An IDE for R. The most straightforward way to get into using R and RMarkdown.\nTeX and LaTeX. A typesetting and document preparation system. You can write files in .tex format directly, but it is more useful to just have it available in the background for other tools to use. The MacTeX Distribution is the one to install for macOS.\nPandoc. Converts plain-text documents to and from a wide variety of formats. Can be installed with Homebrew. Be sure to also install pandoc-citeproc for processing citations and bibliographies, and pandoc-crossref for producing cross-references and labels.\nGit. Version control system. Installs with Apple’s Developer Tools, or get the latest version via Homebrew.\nGitHub. Host public Git repositories for free. Pay to host private ones. Also a source for publicly available code (e.g. R packages and utilities) written by other people.\nGNU Make. You tell make what the steps are to create the pieces of a document or program. As you edit and change the various pieces, it automatically figures out which pieces need to be updated and recompiled, and issues the commands to do that. See Karl Broman’s Minimal Make for a short introduction. Make will be installed automatically with Apple’s developer tools.\nlintr and flycheck. Tools that nudge you to write neater code.\nZotero. A citation manager that incorporates PDF storage, annotation, and other features. Zotero is free to use and can export to BibTeX/BibLaTeX files."
  },
  {
    "objectID": "references.html#paid-applications-and-services",
    "href": "references.html#paid-applications-and-services",
    "title": "References",
    "section": "Paid Applications and Services",
    "text": "Paid Applications and Services\n\nBackblaze. Secure off-site backup.\nMarked 2. Live HTML previewing of Markdown documents. Mac OS X only.\nSublime Text. Python-based text editor.\nMendeley, and Papers are additional citation managers that incorporate PDF storage, annotation, and other features. Mendeley has a premium tier. Papers is a paid application after a trial period. I haven’t used either of these, so I can’t confirm whether or not they export to BibTeX/BibLaTeX files. Papers can supposedly output citation keys in pandoc’s format, among several others."
  },
  {
    "objectID": "references.html#data",
    "href": "references.html#data",
    "title": "References",
    "section": "Data",
    "text": "Data\nMany of these websites offer publicly available datasets that can be used for research or class projects.\n\nHealth and Biological data\n\nCDC National Center for Health Statistics\nNIH Cancer Surveillance\nWorld Health Organization WHO data\nUniProt data\nThe Gene Ontology Project\nGene Expression Omnibus Data\nUS Center for Disease Control and Prevention Data\nCalifornia Health and Human Services Open Data Portal\nCovid Data CovidTracker\nUSC Sustainability Data\nBureau of Transportation Statistics\n\n\n\nAcademic Publications and related\n\nFigshare data repository\nZenodo data repository\nHarvard Dataverse\nElsevier Developers API\n\n\n\nGovernment data\n\nUS Open Data Initiative DATA.GOV\nCensus Data Explorer and National Historical Geographic Information System (NHGIS)\nBureau of Economic Analysis\nBureau of Labor Statistics\nHousing data Zillow\nBureau of Justice Statistics National Center for Education Statistics: The Nation’s Report Card\nLos Angeles city data\nLos Angeles crime data\n\n\n\nOther data\n\nWorld Bank open data\nInter-university Consortium for Political and Social Research (ICPSR)\nFiveThirtyEight open data\nKaggle datasets\nLiterally all of Wikipedia\n\n\n\nSocial Networks\n\nTwitter Developers API (probably broken?)\nGitHub Developers API\nInstagram Developers API\nLinkedIn Developers API"
  },
  {
    "objectID": "labs/01-lab.html",
    "href": "labs/01-lab.html",
    "title": "Lab 01 - Hello R!",
    "section": "",
    "text": "Get acquainted with R and RStudio, which we will be using throughout the course to analyze data.\nAppreciate the value of visualization in exploring the relationship between variables.\nStart using R for building plots and calculating summary statistics."
  },
  {
    "objectID": "labs/01-lab.html#download-r",
    "href": "labs/01-lab.html#download-r",
    "title": "Lab 01 - Hello R!",
    "section": "1 Download R",
    "text": "1 Download R\n\nIf you don’t have R installed.\nGo to the CRAN and download R, make sure you get the version that matches your operating system.\n\n\nIf you have R installed\nIf you have R installed run the following code\n\nR.version\n\n               _                           \nplatform       x86_64-apple-darwin20       \narch           x86_64                      \nos             darwin20                    \nsystem         x86_64, darwin20            \nstatus                                     \nmajor          4                           \nminor          3.3                         \nyear           2024                        \nmonth          02                          \nday            29                          \nsvn rev        86002                       \nlanguage       R                           \nversion.string R version 4.3.3 (2024-02-29)\nnickname       Angel Food Cake             \n\n\nThis should tell you what version of R you are currently using. If your R version is lower then 4.3.0, I would strongly recommend updating. In general, it is a good idea to keep your R version up to date, unless you have a project right now that depends on a specific version of R."
  },
  {
    "objectID": "labs/01-lab.html#download-rstudio",
    "href": "labs/01-lab.html#download-rstudio",
    "title": "Lab 01 - Hello R!",
    "section": "1.2 Download RStudio",
    "text": "1.2 Download RStudio\nWe recommend using RStudio as your IDE if you don’t already have it installed. You can go to the RStudio website to download and install the software. Once it is installed, open RStudio and use it to complete the rest of this lab."
  },
  {
    "objectID": "labs/01-lab.html#start-a-new-project",
    "href": "labs/01-lab.html#start-a-new-project",
    "title": "Lab 01 - Hello R!",
    "section": "Start a new Project",
    "text": "Start a new Project\nRStudio Projects are a great way to stay organized and keep all of your work on a particular topic in one place. Project files keep track of things like the R objects you are using and which files you have open, so that you can quickly jump in and out of different work environments.\nWe’re going to start a new Project called PM566labs. If you want to keep all of your materials for this class in one place, you may want to open Finder (MacOS) or File Explorer (Windows) and create a new folder (directory) for this class.\nIn the top right of the RStudio window, you should see a drop-down menu that says “Project: (None)”. Click on this and then “New Project…”, which will open up a dialogue box. If you had already created a PM566labs directory, you could choose the “Existing Directory” option to associate this Project with that directory. Since we haven’t done that, we’ll use the “New Directory” option, then select the generic Project type, “New Project”. Give your new directory the name PM566labs and use the “Browse” button to choose where you want to save it on your computer, then click “Create Project”. Now in the top right, you should see “PM566labs” next to the R Project logo."
  },
  {
    "objectID": "labs/01-lab.html#create-a-quarto-document",
    "href": "labs/01-lab.html#create-a-quarto-document",
    "title": "Lab 01 - Hello R!",
    "section": "Create a Quarto document",
    "text": "Create a Quarto document\nWe will use Quarto documents a lot in this course because they are fully reproducible and allow us to seamlessly integrate code and text. We expect you to use Quarto for all homework assignments and (almost) all labs.\nIn the top left, you will see a “New File” icon (a white “plus” sign in a green circle over a blank document) that leads to a drop-down menu. Click on this and then select “Quarto document…”, which will open up a dialogue box. You can leave most of the settings on their defaults for now, just give your document a title (like “Lab 1”) and an author (yourself) and click “Create”. RStudio may ask you if you would like to install a package that is required, and if so, click “Install”.\nThis will open the default Quarto document, which already contains some example content. Read through it, then remove this content. Please do not hand in assignments that contain the default content, as I have seen it plenty of times! Set up new sections in your document titled “Question 1” through “Question 6”. This document will serve as the template for your responses to this lab. Under each section title, add an R code chunk via “Insert…”, “Executable Cell”, “R”. Alternatively, you can switch to the “Source” editor (in the top left of the Editor pane) and add an R chunk by typing the following:\n```{r}\n```\nThis will create an R code chunk and any code you add inside of it will be executed when you Render the document.\nSave your Quarto markdown (qmd) file as lab-01.qmd and see what happens when you click “Render”."
  },
  {
    "objectID": "labs/01-lab.html#yaml",
    "href": "labs/01-lab.html#yaml",
    "title": "Lab 01 - Hello R!",
    "section": "YAML",
    "text": "YAML\nOpen the Quarto (qmd) file in your project, make sure the author name is your name, and Render the document."
  },
  {
    "objectID": "labs/01-lab.html#question-1",
    "href": "labs/01-lab.html#question-1",
    "title": "Lab 01 - Hello R!",
    "section": "Question 1",
    "text": "Question 1\n\nBased on the help file, how many rows and how many columns does the datasaurus_dozen file have? What are the variables included in the data frame? Add your responses to your lab report, with relevant code in the associated R code chunk, and free-form text outside of the code chunk.\n\nLet’s take a look at what these datasets are. To start, we can make a frequency table of the dataset variable:\n\ntable(datasaurus_dozen$dataset)\n\nHere, we used the $ operator to access a specific column (variable) of the dataset. Then we used the table function to summarize that variable. table is great for quickly summarizing categorical variables, but it’s not very useful for summarizing continuous variables, where most unique values are only present once. For continuous variables, try the summary function.\nThe original Datasaurus (dino) was created by Alberto Cairo in this great blog post. The other Dozen were generated using simulated annealing and the process is described in the paper Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing by Justin Matejka and George Fitzmaurice. In the paper, the authors simulate a variety of datasets with the same summary statistics as the Datasaurus, but with very different distributions."
  },
  {
    "objectID": "labs/01-lab.html#question-2",
    "href": "labs/01-lab.html#question-2",
    "title": "Lab 01 - Hello R!",
    "section": "Question 2",
    "text": "Question 2\n\nPlot y vs. x for the dino dataset. Then, calculate the correlation coefficient between x and y for just this dataset.\n\nBelow is the code you will need to complete this exercise. Basically, the answer is already given, but you need to include relevant bits in your Quarto document and successfully Render it and view the results.\nWe’re going to start by subsetting our data down to just the dino dataset.\n\nSubsetting\n\ndino_data &lt;- datasaurus_dozen[datasaurus_dozen$dataset == 'dino', ]\n\nThere is a lot going on here, so let’s slow down and unpack it a bit.\nThe first thing to note is the assignment operator: &lt;-. This special symbol is used to create a new object or assign a new value to an existing one. In this case, we’re creating a new object called dino_data.\nThe second important feature is the set of square brackets: []. When these come immediately after the name of an object, they are used for subsetting that object, or taking a smaller piece of it. In this case, we want to subset the datasaurus_dozen object. Since this is a two-dimensional data.frame, we can subset it by either rows or columns, or both. The comma (,) separates our row subset from our column subset. In this case, we only want to subset by row and keep all the columns, so there is nothing after the comma.\nIn R (as in Python and many other programming languages), we can check whether or not two values are equal by using the == operator. Here, we again use the $ to access the dataset variable and we check if it is equal to the value 'dino'. If it is, this comparison will return TRUE and we will keep that row. If it isn’t, this comparison will return FALSE and we will not keep that row.\n\n\nPlotting\nNext, we need to visualize these data. We will use the plot function for this, which is R’s most basic plotting function. If you provide the plot function with two numeric vectors, it will plot them in a scatter plot. Let’s see what happens when we plot the x and y variables from our dino_data object:\n\nplot(dino_data$x, dino_data$y)\n# ggplot(data = dino_data, mapping = aes(x = x, y = y)) +\n#   geom_point()\n\n(I’ve also included code for producing similar plots using the ggplot function from the ggplot2 package. This is another widely-used plotting function that includes some more aesthetically pleasing defaults, but requires more complicated syntax)\nWe will talk a lot more about the philosophy of data visualization, how to choose the right plot type, and constructing visualizations in layers in the coming weeks. But for now, you can follow along with the code that is provided.\nFor the second part of these exercises, we need to calculate a summary statistic: the correlation coefficient. The correlation coefficient, often referred to as \\(r\\) in statistics, measures the linear association between two variables. You will see that some of the pairs of variables we plot do not have a linear relationship between them. This is exactly why we want to visualize first: visualize to assess the form of the relationship, and calculate \\(r\\) only if relevant. In this case, calculating a correlation coefficient really doesn’t make sense since the relationship between x and y is definitely not linear – it’s dinosaurial!\nBut, for illustrative purposes, let’s calculate correlation coefficient between x and y. Like plot, the cor function takes two numeric variables and calculates their correlation coefficient:\n\ncor(dino_data$x, dino_data$y)\n# dino_data |&gt;\n#   summarize(r = cor(x, y))\n\n(I’ve also included the “tidyverse” way of calculating this value. The “tidyverse” is a collection of packages that includes ggplot2 and is based on the concept of “tidy”, rectangular datasets. We will tend to focus on the “Base R” way of doing things, as learning the “tidyverse” coding style is almost like learning another language in addition to R)"
  },
  {
    "objectID": "labs/01-lab.html#question-3",
    "href": "labs/01-lab.html#question-3",
    "title": "Lab 01 - Hello R!",
    "section": "Question 3",
    "text": "Question 3\n\nNow try it on your own! Plot y vs. x for the star dataset, another one of the datasaurus_dozen. You can (and should) re-use code we introduced above, just replace the dataset name with the desired dataset. Then, calculate the correlation coefficient between x and y for this dataset. How does this value compare to the r of dino?"
  },
  {
    "objectID": "labs/01-lab.html#question-4",
    "href": "labs/01-lab.html#question-4",
    "title": "Lab 01 - Hello R!",
    "section": "Question 4",
    "text": "Question 4\n\nPlot y vs. x for the circle dataset. You can (and should) reuse code we introduced above, just replace the dataset name with the desired dataset. Then, calculate the correlation coefficient between x and y for this dataset. How does this value compare to the r of dino?"
  },
  {
    "objectID": "labs/01-lab.html#question-5",
    "href": "labs/01-lab.html#question-5",
    "title": "Lab 01 - Hello R!",
    "section": "Question 5",
    "text": "Question 5\nNow let’s plot all 13 datasets at once. In order to do this we will make use of the layout function. This function allows you to put multiple plots in the same plotting window. We will create a 4x4 matrix containing the values 1 through 16 and pass this to layout, which lets it know where we want each plot to go (we only have 13 datasets, so there will be a few empty spots at the end). This may be too many plots for a small viewing window, so if you get the error message Error in plot.new() : figure margins too large, try making your plotting window larger.\nThen we use a for loop to perform a set of actions over every unique value of the dataset variable. This creates a new object, called name, that takes on each of those unique values, but it only exists within the context (interior) of the loop. Then we subset and plot the data, as we have before.\n\nlayout(matrix(1:16, nrow=4, ncol=4))\nfor(name in unique(datasaurus_dozen$dataset)){\n  subset &lt;- datasaurus_dozen[datasaurus_dozen$dataset == name, ]\n  plot(subset$x, subset$y, main = name)\n}\nlayout(1)\n\n\n# ggplot(datasaurus_dozen, aes(x = x, y = y, color = dataset))+\n#   geom_point()+\n#   facet_wrap(~ dataset, ncol = 3) +\n#   theme(legend.position = \"none\")\n\nThe second call to layout will reset the plotting window, so that the next plot will take up the entire window, rather than 1/16th of it.\nOptional:If you really want to maximize the plotting area of this figure, you can also adjust the plotting margins via the par function (short for “graphical parameters”). The setting for margins is called mar and it takes a vector of length 4, specifying the bottom, left, top, and right margins, in order (the units are lines of text). You can set each margin to a value of 2 with par(mar = c(2,2,2,2)) and then reset to the default values with par(mar = c(5,4,4,2) + 0.1)."
  },
  {
    "objectID": "labs/01-lab.html#question-6",
    "href": "labs/01-lab.html#question-6",
    "title": "Lab 01 - Hello R!",
    "section": "Question 6",
    "text": "Question 6\nFinally, we want to calculate the correlation between the x and y variables for all 13 datasets. Like before, we will use a loop, but this time, since we want to return a specific value every time through the loop, we will use the sapply function. sapply is useful way to apply a function to every element of a vector. In this case, we provide the vector of unique dataset names (like before) and then our own custom function. This function subsets the data as before, and then returns the correlation coefficient as the output of the function.\n\nsapply(unique(datasaurus_dozen$dataset), function(name){\n    subset &lt;- datasaurus_dozen[datasaurus_dozen$dataset == name, ]\n    return(cor(subset$x, subset$y))\n})\n\n\n# datasaurus_dozen |&gt;\n#   group_by(dataset) |&gt;\n#   summarize(r = cor(x, y))\n\nYou’re now done with the data analysis exercises, but we’d like you to do two more things:\nResize your figures:\nAdd the fields fig-width and fig-height to the YAML header of your document. These will allow you to specify the size (in inches) of any figures generated by the code chunks in your report.\nYou can also use different figure sizes for different figures. If you are in the Visual editor mode, switch to Source mode. Notice that each R chunk starts and ends with three backticks. Click on the gear icon in the top right of a code chunk and select “Use custom figure size” in the pop-up menu. Set the height and width of the figures and hit Apply when done. Then, render your document and see how you like the new sizes. Try making the plot for Question 5 larger, until you are happy with its size. Note that changing the figure sizes added new options to these chunks: fig.width and fig.height. You can change them by defining different values directly in your Quarto document as well.\nChange the look of your report:\nIf you have time, you can explore the different ways you can add styling to your document. Try adding a theme field to the YAML header and see if you can find valid names of different themes.\nHere is a Quarto cheatsheet and a general markdown cheatsheet that shows some of the many cool features you can make use of in a Quarto document.\n\nThis set of lab exersices have been adopted from Mine Çetinkaya-Rundel’s class Introduction to Data Science."
  },
  {
    "objectID": "syllabus.html#course-slides",
    "href": "syllabus.html#course-slides",
    "title": "Syllabus",
    "section": "Course Slides",
    "text": "Course Slides\nLecture slides presented in class are available on the course website."
  },
  {
    "objectID": "slides/week1.html",
    "href": "slides/week1.html",
    "title": "Welcome!",
    "section": "",
    "text": "Kelly Street\n\nAssistant Professor of Biostatistics\nDepartment of Population and Public Health Sciences"
  },
  {
    "objectID": "labs/week1.html",
    "href": "labs/week1.html",
    "title": "Lab 1 - Hello R!",
    "section": "",
    "text": "Get acquainted with R and RStudio, which we will be using throughout the course to analyze data.\nAppreciate the value of visualization in exploring the relationship between variables.\nStart using R for building plots and calculating summary statistics."
  },
  {
    "objectID": "labs/week1.html#download-r",
    "href": "labs/week1.html#download-r",
    "title": "Lab 1 - Hello R!",
    "section": "1 Download R",
    "text": "1 Download R\n\nIf you don’t have R installed.\nGo to the CRAN and download R, make sure you get the version that matches your operating system.\n\n\nIf you have R installed\nIf you have R installed run the following code\n\nR.version\n\n               _                           \nplatform       x86_64-apple-darwin20       \narch           x86_64                      \nos             darwin20                    \nsystem         x86_64, darwin20            \nstatus                                     \nmajor          4                           \nminor          3.3                         \nyear           2024                        \nmonth          02                          \nday            29                          \nsvn rev        86002                       \nlanguage       R                           \nversion.string R version 4.3.3 (2024-02-29)\nnickname       Angel Food Cake             \n\n\nThis should tell you what version of R you are currently using. If your R version is lower then 4.3.0, I would strongly recommend updating. In general, it is a good idea to keep your R version up to date, unless you have a project right now that depends on a specific version of R."
  },
  {
    "objectID": "labs/week1.html#download-rstudio",
    "href": "labs/week1.html#download-rstudio",
    "title": "Lab 1 - Hello R!",
    "section": "1.2 Download RStudio",
    "text": "1.2 Download RStudio\nWe recommend using RStudio as your IDE if you don’t already have it installed. You can go to the RStudio website to download and install the software. Once it is installed, open RStudio and use it to complete the rest of this lab."
  },
  {
    "objectID": "labs/week1.html#start-a-new-project",
    "href": "labs/week1.html#start-a-new-project",
    "title": "Lab 1 - Hello R!",
    "section": "Start a new Project",
    "text": "Start a new Project\nRStudio Projects are a great way to stay organized and keep all of your work on a particular topic in one place. Project files keep track of things like the R objects you are using and which files you have open, so that you can quickly jump in and out of different work environments.\nWe’re going to start a new Project called PM566labs. If you want to keep all of your materials for this class in one place, you may want to open Finder (MacOS) or File Explorer (Windows) and create a new folder (directory) for this class.\nIn the top right of the RStudio window, you should see a drop-down menu that says “Project: (None)”. Click on this and then “New Project…”, which will open up a dialogue box. If you had already created a PM566labs directory, you could choose the “Existing Directory” option to associate this Project with that directory. Since we haven’t done that, we’ll use the “New Directory” option, then select the generic Project type, “New Project”. Give your new directory the name PM566labs and use the “Browse” button to choose where you want to save it on your computer, then click “Create Project”. Now in the top right, you should see “PM566labs” next to the R Project logo."
  },
  {
    "objectID": "labs/week1.html#create-a-quarto-document",
    "href": "labs/week1.html#create-a-quarto-document",
    "title": "Lab 1 - Hello R!",
    "section": "Create a Quarto document",
    "text": "Create a Quarto document\nWe will use Quarto documents a lot in this course because they are fully reproducible and allow us to seamlessly integrate code and text. We expect you to use Quarto for all homework assignments and (almost) all labs.\nIn the top left, you will see a “New File” icon (a white “plus” sign in a green circle over a blank document) that leads to a drop-down menu. Click on this and then select “Quarto document…”, which will open up a dialogue box. You can leave most of the settings on their defaults for now, just give your document a title (like “Lab 1”) and an author (yourself) and click “Create”. RStudio may ask you if you would like to install a package that is required, and if so, click “Install”.\nThis will open the default Quarto document, which already contains some example content. Read through it, then remove this content. Please do not hand in assignments that contain the default content, as I have seen it plenty of times! Set up new sections in your document titled “Question 1” through “Question 6”. This document will serve as the template for your responses to this lab. Under each section title, add an R code chunk via “Insert…”, “Executable Cell”, “R”. Alternatively, you can switch to the “Source” editor (in the top left of the Editor pane) and add an R chunk by typing the following:\n```{r}\n```\nThis will create an R code chunk and any code you add inside of it will be executed when you Render the document.\nSave your Quarto markdown (qmd) file as lab-01.qmd and see what happens when you click “Render”."
  },
  {
    "objectID": "labs/week1.html#yaml",
    "href": "labs/week1.html#yaml",
    "title": "Lab 1 - Hello R!",
    "section": "YAML",
    "text": "YAML\nThe top portion of your Quarto file (between the three dashed lines) is called YAML. It stands for “YAML Ain’t Markup Language”. It is a human-friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\nOpen the Quarto (qmd) file in your project, make sure the author name is your name. You can add additional lines to the YAML section to control more aspects of your document. For example, you can set the output to be a PDF or HTML document by adding either format: pdf or format: html.\nRegardless of what output format you choose, add another line to the YAML header that reads embed-resources: true. This tells Quarto that you want to produce a “stand-alone” document that contains all images directly in the document. If you don’t do this, Quarto will often create a separate directory (called &lt;document&gt;_files) that contains these images and your output will depend on this outside directory.\nWhen you’re done, click “Render” to compile the document."
  },
  {
    "objectID": "labs/week1.html#question-1",
    "href": "labs/week1.html#question-1",
    "title": "Lab 1 - Hello R!",
    "section": "Question 1",
    "text": "Question 1\n\nBased on the help file, how many rows and how many columns does the datasaurus_dozen file have? What are the variables included in the data frame? Add your responses to your lab report, with relevant code in the associated R code chunk, and free-form text outside of the code chunk.\n\nLet’s take a look at what these datasets are. To start, we can make a frequency table of the dataset variable:\n\ntable(datasaurus_dozen$dataset)\n\nHere, we used the $ operator to access a specific column (variable) of the dataset. Then we used the table function to summarize that variable. table is great for quickly summarizing categorical variables, but it’s not very useful for summarizing continuous variables, where most unique values are only present once. For continuous variables, try the summary function.\nThe original Datasaurus (dino) was created by Alberto Cairo in this great blog post. The other Dozen were generated using simulated annealing and the process is described in the paper Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing by Justin Matejka and George Fitzmaurice. In the paper, the authors simulate a variety of datasets with the same summary statistics as the Datasaurus, but with very different distributions."
  },
  {
    "objectID": "labs/week1.html#question-2",
    "href": "labs/week1.html#question-2",
    "title": "Lab 1 - Hello R!",
    "section": "Question 2",
    "text": "Question 2\n\nPlot y vs. x for the dino dataset. Then, calculate the correlation coefficient between x and y for just this dataset.\n\nBelow is the code you will need to complete this exercise. Basically, the answer is already given, but you need to include relevant bits in your Quarto document and successfully Render it and view the results.\nWe’re going to start by subsetting our data down to just the dino dataset.\n\nSubsetting\n\ndino_data &lt;- datasaurus_dozen[datasaurus_dozen$dataset == 'dino', ]\n\nThere is a lot going on here, so let’s slow down and unpack it a bit.\nThe first thing to note is the assignment operator: &lt;-. This special symbol is used to create a new object or assign a new value to an existing one. In this case, we’re creating a new object called dino_data.\nThe second important feature is the set of square brackets: []. When these come immediately after the name of an object, they are used for subsetting that object, or taking a smaller piece of it. In this case, we want to subset the datasaurus_dozen object. Since this is a two-dimensional data.frame, we can subset it by either rows or columns, or both. The comma (,) separates our row subset from our column subset. In this case, we only want to subset by row and keep all the columns, so there is nothing after the comma.\nIn R (as in Python and many other programming languages), we can check whether or not two values are equal by using the == operator. Here, we again use the $ to access the dataset variable and we check if it is equal to the value 'dino'. If it is, this comparison will return TRUE and we will keep that row. If it isn’t, this comparison will return FALSE and we will not keep that row.\n\n\nPlotting\nNext, we need to visualize these data. We will use the plot function for this, which is R’s most basic plotting function. If you provide the plot function with two numeric vectors, it will plot them in a scatter plot. Let’s see what happens when we plot the x and y variables from our dino_data object:\n\nplot(dino_data$x, dino_data$y)\n# ggplot(data = dino_data, mapping = aes(x = x, y = y)) +\n#   geom_point()\n\n(I’ve also included code for producing similar plots using the ggplot function from the ggplot2 package. This is another widely-used plotting function that includes some more aesthetically pleasing defaults, but requires more complicated syntax)\nWe will talk a lot more about the philosophy of data visualization, how to choose the right plot type, and constructing visualizations in layers in the coming weeks. But for now, you can follow along with the code that is provided.\nFor the second part of these exercises, we need to calculate a summary statistic: the correlation coefficient. The correlation coefficient, often referred to as \\(r\\) in statistics, measures the linear association between two variables. You will see that some of the pairs of variables we plot do not have a linear relationship between them. This is exactly why we want to visualize first: visualize to assess the form of the relationship, and calculate \\(r\\) only if relevant. In this case, calculating a correlation coefficient really doesn’t make sense since the relationship between x and y is definitely not linear – it’s dinosaurial!\nBut, for illustrative purposes, let’s calculate correlation coefficient between x and y. Like plot, the cor function takes two numeric variables and calculates their correlation coefficient:\n\ncor(dino_data$x, dino_data$y)\n# dino_data |&gt;\n#   summarize(r = cor(x, y))\n\n(I’ve also included the “tidyverse” way of calculating this value. The “tidyverse” is a collection of packages that includes ggplot2 and is based on the concept of “tidy”, rectangular datasets. We will tend to focus on the “Base R” way of doing things, as learning the “tidyverse” coding style is almost like learning another language in addition to R)"
  },
  {
    "objectID": "labs/week1.html#question-3",
    "href": "labs/week1.html#question-3",
    "title": "Lab 1 - Hello R!",
    "section": "Question 3",
    "text": "Question 3\n\nNow try it on your own! Plot y vs. x for the star dataset, another one of the datasaurus_dozen. You can (and should) re-use code we introduced above, just replace the dataset name with the desired dataset. Then, calculate the correlation coefficient between x and y for this dataset. How does this value compare to the r of dino?"
  },
  {
    "objectID": "labs/week1.html#question-4",
    "href": "labs/week1.html#question-4",
    "title": "Lab 1 - Hello R!",
    "section": "Question 4",
    "text": "Question 4\n\nPlot y vs. x for the circle dataset. You can (and should) reuse code we introduced above, just replace the dataset name with the desired dataset. Then, calculate the correlation coefficient between x and y for this dataset. How does this value compare to the r of dino?"
  },
  {
    "objectID": "labs/week1.html#question-5",
    "href": "labs/week1.html#question-5",
    "title": "Lab 1 - Hello R!",
    "section": "Question 5",
    "text": "Question 5\nNow let’s plot all 13 datasets at once. In order to do this we will make use of the layout function. This function allows you to put multiple plots in the same plotting window. We will create a 4x4 matrix containing the values 1 through 16 and pass this to layout, which lets it know where we want each plot to go (we only have 13 datasets, so there will be a few empty spots at the end). This may be too many plots for a small viewing window, so if you get the error message Error in plot.new() : figure margins too large, try making your plotting window larger.\nThen we use a for loop to perform a set of actions over every unique value of the dataset variable. This creates a new object, called name, that takes on each of those unique values, but it only exists within the context (interior) of the loop. Then we subset and plot the data, as we have before.\n\nlayout(matrix(1:16, nrow=4, ncol=4))\nfor(name in unique(datasaurus_dozen$dataset)){\n  subset &lt;- datasaurus_dozen[datasaurus_dozen$dataset == name, ]\n  plot(subset$x, subset$y, main = name)\n}\nlayout(1)\n\n\n# ggplot(datasaurus_dozen, aes(x = x, y = y, color = dataset))+\n#   geom_point()+\n#   facet_wrap(~ dataset, ncol = 3) +\n#   theme(legend.position = \"none\")\n\nThe second call to layout will reset the plotting window, so that the next plot will take up the entire window, rather than 1/16th of it.\nOptional:If you really want to maximize the plotting area of this figure, you can also adjust the plotting margins via the par function (short for “graphical parameters”). The setting for margins is called mar and it takes a vector of length 4, specifying the bottom, left, top, and right margins, in order (the units are lines of text). You can set each margin to a value of 2 with par(mar = c(2,2,2,2)) and then reset to the default values with par(mar = c(5,4,4,2) + 0.1)."
  },
  {
    "objectID": "labs/week1.html#question-6",
    "href": "labs/week1.html#question-6",
    "title": "Lab 1 - Hello R!",
    "section": "Question 6",
    "text": "Question 6\nFinally, we want to calculate the correlation between the x and y variables for all 13 datasets. Like before, we will use a loop, but this time, since we want to return a specific value every time through the loop, we will use the sapply function. sapply is useful way to apply a function to every element of a vector. In this case, we provide the vector of unique dataset names (like before) and then our own custom function. This function subsets the data as before, and then returns the correlation coefficient as the output of the function.\n\nsapply(unique(datasaurus_dozen$dataset), function(name){\n    subset &lt;- datasaurus_dozen[datasaurus_dozen$dataset == name, ]\n    return(cor(subset$x, subset$y))\n})\n\n\n# datasaurus_dozen |&gt;\n#   group_by(dataset) |&gt;\n#   summarize(r = cor(x, y))\n\nYou’re now done with the data analysis exercises, but we’d like you to do two more things:\nResize your figures:\nAdd the fields fig-width and fig-height to the YAML header of your document. These will allow you to specify the size (in inches) of any figures generated by the code chunks in your report.\nYou can also use different figure sizes for different figures. If you are in the Visual editor mode, switch to Source mode. Notice that each R chunk starts and ends with three backticks. Click on the gear icon in the top right of a code chunk and select “Use custom figure size” in the pop-up menu. Set the height and width of the figures and hit Apply when done. Then, render your document and see how you like the new sizes. Try making the plot for Question 5 larger, until you are happy with its size. Note that changing the figure sizes added new options to these chunks: fig.width and fig.height. You can change them by defining different values directly in your Quarto document as well.\nChange the look of your report:\nIf you have time, you can explore the different ways you can add styling to your document. Try adding a theme field to the YAML header and see if you can find valid names of different themes.\nHere is a Quarto cheatsheet and a general markdown cheatsheet that shows some of the many cool features you can make use of in a Quarto document."
  },
  {
    "objectID": "slides/week2.html#what-is-version-control",
    "href": "slides/week2.html#what-is-version-control",
    "title": "Version control and reproducible research",
    "section": "What is version control?",
    "text": "What is version control?\n\n\n\n\n\n[I]s the management of changes to documents […] Changes are usually identified by a number or letter code, termed the “revision number”, “revision level”, or simply “revision”. For example, an initial set of files is “revision 1”. When the first change is made, the resulting set is “revision 2”, and so on. Each revision is associated with a timestamp and the person making the change. Revisions can be compared, restored, and with some types of files, merged. – Wikipedia"
  },
  {
    "objectID": "slides/week2.html#why-do-we-care",
    "href": "slides/week2.html#why-do-we-care",
    "title": "Version control and reproducible research",
    "section": "Why do we care",
    "text": "Why do we care\nHave you ever:\n\nMade a change to code, realised it was a mistake and wanted to revert back?\nLost code or had a backup that was too old?\nHad to maintain multiple versions of a product?\nWanted to see the difference between two (or more) versions of your code?\nWanted to prove that a particular change broke or fixed a piece of code?\nWanted to review the history of some code?"
  },
  {
    "objectID": "slides/week2.html#why-do-we-care-contd",
    "href": "slides/week2.html#why-do-we-care-contd",
    "title": "Version control and reproducible research",
    "section": "Why do we care (cont’d)",
    "text": "Why do we care (cont’d)\n\nWanted to submit a change to someone else’s code?\nWanted to share your code, or let other people work on your code?\nWanted to see how much work is being done, and where, when and by whom?\nWanted to experiment with a new feature without interfering with working code?\n\nIn these cases, and no doubt others, a version control system should make your life easier.\n– Stackoverflow (by si618)"
  },
  {
    "objectID": "slides/week2.html#git-the-stupid-content-tracker",
    "href": "slides/week2.html#git-the-stupid-content-tracker",
    "title": "Version control and reproducible research",
    "section": "Git: The stupid content tracker",
    "text": "Git: The stupid content tracker\n\n\n \n\n\nGit logo and Linus Torvalds, creator of git"
  },
  {
    "objectID": "slides/week2.html#how-can-i-use-git",
    "href": "slides/week2.html#how-can-i-use-git",
    "title": "Version control and reproducible research",
    "section": "How can I use Git",
    "text": "How can I use Git\nThere are several ways to include Git in your work-pipeline. A few are:\n\nThrough command line\nThrough one of the available Git GUIs:\n\nRStudio (link)\nGit-Cola (link)\nGithub Desktop (Link)\n\n\nMore alternatives here."
  },
  {
    "objectID": "slides/week2.html#a-common-workflow",
    "href": "slides/week2.html#a-common-workflow",
    "title": "Version control and reproducible research",
    "section": "A Common workflow",
    "text": "A Common workflow\n\nStart the session by pulling (possible) updates: git pull\nMake changes\n\n(optional) Add untracked (possibly new) files: git add [target file]\n(optional) Stage tracked files that were modified: git add [target file]\n(optional) Revert changes on a file: git checkout [target file]\n\nMove changes to the staging area (optional): git add\nCommit:\n\nIf nothing pending: git commit -m \"Your comments go here.\"\nIf modifications not staged: git commit -a -m \"Your comments go here.\"\n\nUpload the commit to the remote repo: git push."
  },
  {
    "objectID": "slides/week2.html#a-common-workflow-1",
    "href": "slides/week2.html#a-common-workflow-1",
    "title": "Version control and reproducible research",
    "section": "A Common workflow",
    "text": "A Common workflow\n\nStart the session by pulling (possible) updates: git pull\nMake changes\n\n\n(optional) Add untracked (possibly new) files: git add [target file]\n(optional) Stage tracked files that were modified: git add [target file]\n(optional) Revert changes on a file: git checkout [target file]\n\n\nMove changes to the staging area (optional): git add\nCommit:\n\n\nIf nothing pending: git commit -m \"Your comments go here.\"\nIf modifications not staged: git commit -a -m \"Your comments go here.\"\n\n\nUpload the commit to the remote repo: git push."
  },
  {
    "objectID": "slides/week2.html#hands-on-0-introduce-yourself",
    "href": "slides/week2.html#hands-on-0-introduce-yourself",
    "title": "Version control and reproducible research",
    "section": "Hands-on 0: Introduce yourself",
    "text": "Hands-on 0: Introduce yourself\nSet up your git install with git config, start by telling who you are\n$ git config --global user.name \"Juan Perez\"\n$ git config --global user.email \"jperez@treschanchitos.edu\"\nTry it yourself (5 minutes) (more on how to configure git here)"
  },
  {
    "objectID": "slides/week2.html#hands-on-1-local-repository",
    "href": "slides/week2.html#hands-on-1-local-repository",
    "title": "Version control and reproducible research",
    "section": "Hands-on 1: Local repository",
    "text": "Hands-on 1: Local repository\nOops! It seems that I added the wrong file to the tree, you can remove files from the tree using git rm --cached, for example, imagine that you added the file class-notes.docx (which you are not supposed to track), then you can remove it using\n$ git rm --cached class-notes.docx\nThis will remove the file from the tree but not from your computer. You can go further and ask git to avoid adding .docx files using the .gitignore file"
  },
  {
    "objectID": "slides/week2.html#hands-on-1-local-repository-solution",
    "href": "slides/week2.html#hands-on-1-local-repository-solution",
    "title": "Version control and reproducible research",
    "section": "Hands-on 1: Local repository (solution)",
    "text": "Hands-on 1: Local repository (solution)\nThe following code is fully executable (copy-pastable)\n# (a) Creating the folder for the project (and getting in there)\nmkdir ~/PM-566-first-project\ncd ~/PM-566-first-project\n\n# (b) Initializing git, creating a file, and adding the file\ngit init\n\n# (c) Creating the Readme file\necho An empty line &gt; README.md\n\n# (d) Adding the file to the tree\ngit add README.md\ngit status\n\n# (e) Commiting and checkout out the history\ngit commit -m \"My first commit ever!\"\ngit log"
  },
  {
    "objectID": "slides/week2.html#hands-on-1-local-repository-1",
    "href": "slides/week2.html#hands-on-1-local-repository-1",
    "title": "Version control and reproducible research",
    "section": "Hands-on 1: Local repository",
    "text": "Hands-on 1: Local repository\nUps! It seems that I added the wrong file to the tree, you can remove files from the tree using git rm --cached, for example, imagine that you added the file class-notes.docx (which you are not supposed to track), then you can remove it using\n$ git rm --cached class-notes.docx\nThis will remove the file from the tree but not from your computer. You can go further and ask git to avoid adding docx files using the .gitignore file"
  },
  {
    "objectID": "slides/week2.html#hands-on-2-remote-repository",
    "href": "slides/week2.html#hands-on-2-remote-repository",
    "title": "Version control and reproducible research",
    "section": "Hands-on 2: Remote repository",
    "text": "Hands-on 2: Remote repository\nNow that you have something to share, your teammates are asking you to share the code with them. Since you are smart, you know you can do this using something like Gitlab or Github. So you now need to:\n\nCreate an online repository (empty) for your project using Github.\nAdd the remote using git remote add, in particular\n\n$ git remote add origin https://github.com/[your user name]/PM-566-first-project.git\nThe use the commands git status and git remote -v to see what’s going on.\n\nPush the changes to the remote using git branch and git push like this:\n\n$ git branch -M main\n$ git push -u origin main\nYou should also check the status of the project using git status to see what git tells you about it. Origin is the tag associated with the remote repo you set up, while main is the tag associated with the current branch of your repo.\nRecommended: Complete GitHub’s Training team “Uploading your project to GitHub”"
  },
  {
    "objectID": "slides/week2.html#hands-on-2-remote-repository-solutions-a",
    "href": "slides/week2.html#hands-on-2-remote-repository-solutions-a",
    "title": "Version control and reproducible research",
    "section": "Hands-on 2: Remote repository (solutions a)",
    "text": "Hands-on 2: Remote repository (solutions a)\n\n\n&lt;img style=“width: 800px;vertical-align: middle;” hspace=“20px” src=“fig/new-github-repo-step1.png” alt=“New GitHub repo”"
  },
  {
    "objectID": "slides/week2.html#hands-on-2-remote-repository-solutions-a-1",
    "href": "slides/week2.html#hands-on-2-remote-repository-solutions-a-1",
    "title": "Version control and reproducible research",
    "section": "Hands-on 2: Remote repository (solutions a)",
    "text": "Hands-on 2: Remote repository (solutions a)\n\n\n&lt;img style=“width: 600px;vertical-align: middle;” hspace=“20px” src=“fig/new-github-repo-step2.png” alt=“New GitHub repo 2”"
  },
  {
    "objectID": "slides/week2.html#hands-on-2-remote-repository-solutions-b",
    "href": "slides/week2.html#hands-on-2-remote-repository-solutions-b",
    "title": "Version control and reproducible research",
    "section": "Hands-on 2: Remote repository (solutions b)",
    "text": "Hands-on 2: Remote repository (solutions b)\nFor part (b), there are a couple of solutions, first, you could try using your ssh-key (if you set it up)\n# (b)\ngit remote add origin git@github.com:gvegayon/PM-566-first-project.git\ngit remote -v\ngit status\nOtherwise, you can use the simple URL (this will prompt user+password) every time you want to push (and pull, if private). When asked for your password, GitHub actually wants your Personal access token.\n# (b)\ngit remote add origin https://github.com/gvegayon/PM-566-first-project.git\ngit remote -v\ngit status"
  },
  {
    "objectID": "slides/week2.html#hands-on-2-remote-repository-solutions-c",
    "href": "slides/week2.html#hands-on-2-remote-repository-solutions-c",
    "title": "Version control and reproducible research",
    "section": "Hands-on 2: Remote repository (solutions c)",
    "text": "Hands-on 2: Remote repository (solutions c)\nThe command git branch, will create a branch named main.\nFor the first git push, you need to specify source (main) and target (origin) and set the upstream (the -u option):\n# (c)\n git branch -M main\n git push -u origin main\n git status\nThe --set-upstream, which was invoked with -u, will set the tracking reference for pull and push."
  },
  {
    "objectID": "slides/week2.html#example-for-.gitignore",
    "href": "slides/week2.html#example-for-.gitignore",
    "title": "Version control and reproducible research",
    "section": "Example for .gitignore",
    "text": "Example for .gitignore\nTelling git to ignore files is a good way to make sure you don’t go over your storage limit on GitHub. It’s also just a convenient way to avoid unnecessary clutter. Example based on Pro-Git (link).\n# ignore specific file (something.pdf)\nsomething.pdf\n\n# ignore all .png files\n*.png\n\n# but do track bird.png, even though you're ignoring .png files\n!bird.png\n\n# only ignore the TODO file in the root directory, not subdir/TODO\n/TODO\n\n# ignore all files in any directory named build\nbuild/\n\n# ignore doc/notes.txt, but not doc/server/arch.txt\ndoc/*.txt\n\n# ignore all .pdf files in the doc/ directory and any of its subdirectories\ndoc/**/*.pdf"
  },
  {
    "objectID": "slides/week2.html#git-the-stupid-content-tracker-1",
    "href": "slides/week2.html#git-the-stupid-content-tracker-1",
    "title": "Version control and reproducible research",
    "section": "Git: The stupid content tracker",
    "text": "Git: The stupid content tracker\n\nDuring this class (and perhaps, the entire program) we will be using Git.\nGit is used by most developers in the world.\nA great reference about the tool can be found here\nMore on what’s stupid about git here."
  },
  {
    "objectID": "slides/week2.html#basic-commands",
    "href": "slides/week2.html#basic-commands",
    "title": "Version control and reproducible research",
    "section": "Basic commands",
    "text": "Basic commands"
  },
  {
    "objectID": "slides/week2.html#first-time",
    "href": "slides/week2.html#first-time",
    "title": "Version control and reproducible research",
    "section": "First time",
    "text": "First time"
  },
  {
    "objectID": "slides/week2.html#the-rest-of-the-time",
    "href": "slides/week2.html#the-rest-of-the-time",
    "title": "Version control and reproducible research",
    "section": "The rest of the time",
    "text": "The rest of the time"
  },
  {
    "objectID": "slides/week2.html#check-the-status",
    "href": "slides/week2.html#check-the-status",
    "title": "Version control and reproducible research",
    "section": "Check the status",
    "text": "Check the status\n\nCan’t remember if you’ve changed any files?\nDon’t know if your local repository is in sync with the remote repository?\n\nYou can always check the current state of your repository with git status!"
  },
  {
    "objectID": "slides/week2.html#hands-on-1-first-repository",
    "href": "slides/week2.html#hands-on-1-first-repository",
    "title": "Version control and reproducible research",
    "section": "Hands-on 1: First repository",
    "text": "Hands-on 1: First repository\nWe will start by working on our very first project. To do so, you are required to start using Git and Github so you can share your code with your team. For this exercise, you need to\n\nLog into GitHub and click on the “plus” icon (“Create new…”) in the top right, then select “New repository”\nGive your repo a name, like PM566-first-project, tell GitHub to add a README file, and click “Create repository”\nFrom your repository’s page, click the green “Code” button and copy the URL, which should end with .git\nFrom the command line on your computer, type git clone and then paste the URL\n\nYou now have a local version of your repository!"
  },
  {
    "objectID": "slides/week2.html#hands-on-1-first-repository-1",
    "href": "slides/week2.html#hands-on-1-first-repository-1",
    "title": "Version control and reproducible research",
    "section": "Hands-on 1: First repository",
    "text": "Hands-on 1: First repository\nNow, let’s make some changes!\n\nOpen the README file in a text editor and add a brief description of the project (this doesn’t have to be accurate, just add some text), then save your changes. If you check the git status now, you’ll see that you have unstaged changes.\nAdd your changes to the staging area with git add README or git add --all. If you check the git status now, you’ll see that you have staged changes, ready to commit.\n\nNote 1: We are assuming that you already installed git in your system.\nNote 2: Need a text editor? Checkout this website link."
  },
  {
    "objectID": "slides/week2.html#hands-on-1-first-repository-2",
    "href": "slides/week2.html#hands-on-1-first-repository-2",
    "title": "Version control and reproducible research",
    "section": "Hands-on 1: First repository",
    "text": "Hands-on 1: First repository\n\nMake the first commit using the git commit command adding a message, e.g.\n\n$ git commit -m \"My first commit ever!\"\nIf you check the git status now, you’ll see that you are 1 commit ahead of the remote repository (GitHub).\n\nUpdate your remote repository (on GitHub) with git push. If you check the git status now, you should see that you are fully up to date.\nIn your browser, refresh the page for your repository and see if your changes to the README file are there!"
  },
  {
    "objectID": "slides/week2.html#hands-on-1-first-repository-3",
    "href": "slides/week2.html#hands-on-1-first-repository-3",
    "title": "Version control and reproducible research",
    "section": "Hands-on 1: First repository",
    "text": "Hands-on 1: First repository\nOops! It seems that I added the wrong file to the tree, you can remove files from the tree using git rm --cached, for example, imagine that you added the file class-notes.docx (which you are not supposed to track), then you can remove it using\n$ git rm --cached class-notes.docx\nThis will remove the file from the tree but not from your computer. You can go further and ask git to avoid adding .docx files using the .gitignore file"
  },
  {
    "objectID": "slides/week2.html#gitignore-use-case",
    "href": "slides/week2.html#gitignore-use-case",
    "title": "Version control and reproducible research",
    "section": ".gitignore use-case",
    "text": ".gitignore use-case\nI like to have my data and code for a project all in the same place, but I don’t want to upload the data to GitHub, as this would exceed the size limit on a repository.\nOpen (or create) the .gitignore file in a text editor and add the following line to ignore the directory called data:\ndata/"
  },
  {
    "objectID": "slides/week2.html#lets-do-the-same-sequence-of-tasks-we-just-performed-but-this-time-using-github-desktop.",
    "href": "slides/week2.html#lets-do-the-same-sequence-of-tasks-we-just-performed-but-this-time-using-github-desktop.",
    "title": "Version control and reproducible research",
    "section": "Let’s do the same sequence of tasks we just performed, but this time, using GitHub Desktop.",
    "text": "Let’s do the same sequence of tasks we just performed, but this time, using GitHub Desktop."
  },
  {
    "objectID": "labs/lab2.html",
    "href": "labs/lab2.html",
    "title": "Lab 02 - GitHub",
    "section": "",
    "text": "In this lab, you are expected to learn/put in practice the following skills:\n\nForking a repository on GitHub\nGit workflow clone/commit/push\nUsing pull requests (PR)"
  },
  {
    "objectID": "labs/lab2.html#step-1-fork-the-project-to-create-your-own-repo",
    "href": "labs/lab2.html#step-1-fork-the-project-to-create-your-own-repo",
    "title": "Lab 02 - GitHub",
    "section": "Step 1: Fork the project to create your own repo",
    "text": "Step 1: Fork the project to create your own repo\nNot a term/command actually available in Git, forking is a feature available in Github (as in other services) that allows users to create copies of other people’s projects to propose changes (i.e. make pull requests, i.e. “I have this great update for your project! Would you like to add it by pulling it into your repo?”).\nTo start, you just need to use the Fork button available on the main page of the repository you would like to contribute to2:\nOnce you “Fork” a project, GitHub will automatically:\n\nCreate a copy (using git clone) of that project in your account.\nSet up a pipeline to generate pull requests for the original repository.\n\nOnce you have a copy of the project in your account, you can procede by “downloading it” to your computer using the git clone command. You will need to copy the URL from your version of the repository, available under the “Code” button. For example, if your github user name is statsnerd and the repository name is PM566-whoami, you could use the following in your command line\ncd where/you/want/to/download/the/thing\ngit clone https://github.com/statsnerd/PM566-whoami.git\nAnd if you have your ssh credentials set up, you can do instead\ncd where/you/want/to/download/the/thing\ngit clone git@github.com:statsnerd/PM566-whoami.git\nThis way you will get a copy of the repository in your local machine. Now, let’s see how can we update the project!"
  },
  {
    "objectID": "labs/lab2.html#step-2-modifying-the-corresponding-line",
    "href": "labs/lab2.html#step-2-modifying-the-corresponding-line",
    "title": "Lab 02 - GitHub",
    "section": "Step 2: Modifying the corresponding line",
    "text": "Step 2: Modifying the corresponding line\nIf you got the correct copy, you should find a very simple repository with only two files: CODE_OF_CONDUCT.md and README.md. The first file is a general code of conduct for the project, which we do not need to edit. The second file is the one that we will be playing with. The README file, which happens to be a Markdown file, contains, or at least will contain, your and your team members’ biographies. Here is what you need to do:\n\nFind the line with your name.\nIn that single line (i.e. not spanning multiple lines), write something about yourself, e.g. “I am from XYZ, I love doing ABC, …”.\n(optional) if you feel like it, add at the end of the line a picture of yourself (or avatar) using either html or markdown. This will require you to include the figure in the repo (if you are not linking a web fig).\nCommit the changes and push the changes to your repo using git commit and git push, e.g.\n\ngit commit -a -m \"[A short but meaningful message]\"\n# git add [your-avatar.png] ... if you need to add a picture\ngit push\nYou have now updated your online version of the PM566-whoami repo and are one step closer to make your first Pull Request. We will see how that happens in the next part."
  },
  {
    "objectID": "labs/lab2.html#step-3-do-the-pull-request",
    "href": "labs/lab2.html#step-3-do-the-pull-request",
    "title": "Lab 02 - GitHub",
    "section": "Step 3: Do the pull request",
    "text": "Step 3: Do the pull request\nThis is the final step. Overall, pull requests (PRs) are as complex as the proposed changes are. The PR that you are about to make should go smoothly, yet, any time that you make a new PR, the changes should be able to be merged in the original repository without conflicts. Conflicts may only appear if the proposed changes are out-dated with respect to the main repository, meaning that the main repository was modified after your fork and your proposed changes cannot be merged without generating conflicts3. For now, let’s just look at the simple case.\nTo create the PR, you just need to go to your online copy of the project and click on “Contribute” then “Open Pull Request”:\n\n\n\nYou can submit pull requests to the original repo from your copy of it via the “Contribute” button.\n\n\nThis will create a PR in the original repository. GitHub will automatically analyze the PR and check whether merging the PR to the master branch will result in a conflict or not. If all is OK, then the owner/admin of the repository can merge the PR. Otherwise, if there’s a conflict, you can go back to your local repo, make the needed changes, commit the changes, and push the changes to your copy on GitHub. In this stage, the PR will automatically update to reflect the new changes you made in your copy of the project.\nFor more information, check out Creating a pull request from a fork on GitHub."
  },
  {
    "objectID": "labs/lab2.html#footnotes",
    "href": "labs/lab2.html#footnotes",
    "title": "Lab 02 - GitHub",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTeam-members could be working on the same file but editing different lines of code. If this is the case, after pull/push, git will integrate the changes without conflicts.↩︎\nFor more details, take a look at the Forking Projects article in GitHub guides.↩︎\nMore info about how to deal with conflicts in this very neat post on stackoverflow.com How to resolve merge conflicts in Git. GitHub also has a way to solve conflicts in PRs, but this is only available to the admins of target repo. More info here,↩︎"
  },
  {
    "objectID": "labs/lab2.html#submitting-your-lab",
    "href": "labs/lab2.html#submitting-your-lab",
    "title": "Lab 02 - GitHub",
    "section": "Submitting your lab",
    "text": "Submitting your lab\nOnce you have created a Pull Request and it has been accepted, you should see your text on the original repository (not just your personal forked copy). If you can see your text on the USCbiostats version, then congratulations, you’re done with Lab 2!"
  },
  {
    "objectID": "slides/week3.html#acknowledgment",
    "href": "slides/week3.html#acknowledgment",
    "title": "Exploratory Data Analysis",
    "section": "Acknowledgment",
    "text": "Acknowledgment\nThese slides were originally developed by Meredith Franklin. They have been modified by George G. Vega Yon and Kelly Street."
  },
  {
    "objectID": "slides/week3.html#exploratory-data-analysis",
    "href": "slides/week3.html#exploratory-data-analysis",
    "title": "Exploratory Data Analysis",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nExploratory data analysis is the process of becoming familiar with a dataset\nIt should be the first step in your analysis pipeline\nIt involves:\n\nchecking data (import issues, outliers, missing values, data errors)\ncleaning data\nsummary statistics of key variables (univariate and bivariate)\nbasic plots and graphs"
  },
  {
    "objectID": "slides/week3.html#pipeline",
    "href": "slides/week3.html#pipeline",
    "title": "Exploratory Data Analysis",
    "section": "Pipeline",
    "text": "Pipeline\n\nEDA involves Import -&gt; Tidy -&gt; Transform -&gt; Visualize. Basically it is everything before we do modeling, prediction or inference.\nEDA may involve some statistical summaries, but it does not include formal statistical analysis."
  },
  {
    "objectID": "slides/week3.html#eda-checklist",
    "href": "slides/week3.html#eda-checklist",
    "title": "Exploratory Data Analysis",
    "section": "EDA Checklist",
    "text": "EDA Checklist\nThe goal of EDA is to better understand your data. Let’s use the checklist:\n\nFormulate a question\nRead in the data\nCheck the dimensions and headers and footers of the data\nCheck the variable types in the data\nInspect the distributions of some/all of the variables\nValidate with an external source\nCompute summary statistics to begin answering the question\nMake exploratory graphs"
  },
  {
    "objectID": "slides/week3.html#the-goal-of-eda-is-to-better-understand-your-data.-lets-use-the-checklist",
    "href": "slides/week3.html#the-goal-of-eda-is-to-better-understand-your-data.-lets-use-the-checklist",
    "title": "Exploratory Data Analysis",
    "section": "The goal of EDA is to better understand your data. Let’s use the checklist:",
    "text": "The goal of EDA is to better understand your data. Let’s use the checklist:"
  },
  {
    "objectID": "slides/week3.html#formulate-a-question",
    "href": "slides/week3.html#formulate-a-question",
    "title": "Exploratory Data Analysis",
    "section": "Formulate a Question",
    "text": "Formulate a Question\nIt is a good idea to first have a question such as:\n\nWhich weather stations reported the hottest and coldest daily temperatures?\nWhat day of the month was on average the hottest?\nIs there correlation between temperature and humidity in my dataset?"
  },
  {
    "objectID": "slides/week3.html#read-in-the-data",
    "href": "slides/week3.html#read-in-the-data",
    "title": "Exploratory Data Analysis",
    "section": "Read in the Data",
    "text": "Read in the Data\nThere are several ways to read in data (some depend on the type of data you have):\n\nread.table or read.csv in base R for delimited files\nreadRDS if you have a .rds dataset (this is a handy, compressed way of saving R objects)\nread_csv, read_csv2, read_delim, read_fwf from library(readr) that is part of the tidyverse\nreadxl() from library(readxl) for .xls and .xlsx files\nread_sas, read_spss, read_stata from library(haven)\nfread from library(data.table) for efficiently importing large datasets that are regular delimited files"
  },
  {
    "objectID": "slides/week3.html#check-the-dimensions-and-headers-and-footers-of-the-data",
    "href": "slides/week3.html#check-the-dimensions-and-headers-and-footers-of-the-data",
    "title": "Exploratory Data Analysis",
    "section": "3. Check the dimensions and headers and footers of the data",
    "text": "3. Check the dimensions and headers and footers of the data"
  },
  {
    "objectID": "slides/week3.html#check-the-variable-types-in-the-data",
    "href": "slides/week3.html#check-the-variable-types-in-the-data",
    "title": "Exploratory Data Analysis",
    "section": "4. Check the variable types in the data",
    "text": "4. Check the variable types in the data"
  },
  {
    "objectID": "slides/week3.html#take-a-closer-look-at-someall-of-the-variables",
    "href": "slides/week3.html#take-a-closer-look-at-someall-of-the-variables",
    "title": "Exploratory Data Analysis",
    "section": "5. Take a closer look at some/all of the variables",
    "text": "5. Take a closer look at some/all of the variables"
  },
  {
    "objectID": "slides/week3.html#validate-with-an-external-source",
    "href": "slides/week3.html#validate-with-an-external-source",
    "title": "Exploratory Data Analysis",
    "section": "6. Validate with an external source",
    "text": "6. Validate with an external source"
  },
  {
    "objectID": "slides/week3.html#conduct-some-summary-statistics-to-answer-the-initial-question",
    "href": "slides/week3.html#conduct-some-summary-statistics-to-answer-the-initial-question",
    "title": "Exploratory Data Analysis",
    "section": "7. Conduct some summary statistics to answer the initial question",
    "text": "7. Conduct some summary statistics to answer the initial question\n\nMake exploratory graphs"
  },
  {
    "objectID": "slides/week3.html#case-study",
    "href": "slides/week3.html#case-study",
    "title": "Exploratory Data Analysis",
    "section": "Case study",
    "text": "Case study\nWe are going to use a dataset created from the National Center for Environmental Information (https://www.ncei.noaa.gov/). The data are 2019 hourly measurements from weather stations across the continental U.S."
  },
  {
    "objectID": "slides/week3.html#formulate-a-question-1",
    "href": "slides/week3.html#formulate-a-question-1",
    "title": "Exploratory Data Analysis",
    "section": "Formulate a Question",
    "text": "Formulate a Question"
  },
  {
    "objectID": "slides/week3.html#it-is-a-good-idea-to-first-have-a-question-such-as",
    "href": "slides/week3.html#it-is-a-good-idea-to-first-have-a-question-such-as",
    "title": "Exploratory Data Analysis",
    "section": "It is a good idea to first have a question such as:",
    "text": "It is a good idea to first have a question such as:\n\nwhat weather stations reported the hottest and coldest daily temperatures?\nwhat day of the month was on average the hottest?\nis there covariation between temperature and humidity in my dataset?"
  },
  {
    "objectID": "slides/week3.html#read-in-the-data-1",
    "href": "slides/week3.html#read-in-the-data-1",
    "title": "Exploratory Data Analysis",
    "section": "Read in the Data",
    "text": "Read in the Data\nThere are plenty of ways to do these tasks, but we will focus on base R.\nSince our data is stored as a (gzipped) CSV file, we could load it into R with read.csv, but we will use the more flexible read.table. I have it stored locally, but we will see how to load it straight from GitHub in the lab.\n\nmet &lt;- read.table(file.path('..','..','data','met_all.gz'),\n                  header = TRUE, sep = ',')\n\nWe specify that the first line contains column names by setting header = TRUE and we indicate that commas are used to separate the different values (rather than tabs, spaces, etc.) by setting sep = ','."
  },
  {
    "objectID": "slides/week3.html#read-in-the-data-2",
    "href": "slides/week3.html#read-in-the-data-2",
    "title": "Exploratory Data Analysis",
    "section": "Read in the Data",
    "text": "Read in the Data\ndata.table is a more efficient version of base R’s data.frame. We create a data.table from an external data source by reading it in using fread()\nWe can convert existing data.frame or list objects to data.table by using setDT()\nThe nice thing about data.table is that it handles large datasets efficiently and it never reads/converts character type variables to factors, which can be a pain with data.frame\nThe other nice thing about data.table is that many of the base R functions work just as if it was a data.frame"
  },
  {
    "objectID": "slides/week3.html#read-in-the-data-3",
    "href": "slides/week3.html#read-in-the-data-3",
    "title": "Exploratory Data Analysis",
    "section": "Read in the Data",
    "text": "Read in the Data\ndata.table is a more efficient version of base R’s data.frame. We create a data.table from an external data source by reading it in using fread()\nWe can convert existing data.frame or list objects to data.table by using setDT()\nThe nice thing about data.table is that it handles large datasets efficiently and it never reads/converts character type variables to factors, which can be a pain with data.frame\nThe other nice thing about data.table is that many of the base R functions work just as if it was a data.frame"
  },
  {
    "objectID": "slides/week3.html#check-the-data",
    "href": "slides/week3.html#check-the-data",
    "title": "Exploratory Data Analysis",
    "section": "Check the data",
    "text": "Check the data\nWe should check the dimensions of the data set. This can be done several ways:\n\ndim(met)\n\n[1] 2377343      30\n\nnrow(met)\n\n[1] 2377343\n\nncol(met)\n\n[1] 30"
  },
  {
    "objectID": "slides/week3.html#check-the-data-1",
    "href": "slides/week3.html#check-the-data-1",
    "title": "Exploratory Data Analysis",
    "section": "Check the data",
    "text": "Check the data\n\nWe see that there are 2,377,343 records of hourly temperature in August 2019 from all of the weather stations in the US. The data set has 30 variables.\nWe should also check the top and bottom of the dataset to check for any irregularities. Use head(met) and tail(met) for this.\nNext we can take a deeper dive into the contents of the data with str()"
  },
  {
    "objectID": "slides/week3.html#check-variables",
    "href": "slides/week3.html#check-variables",
    "title": "Exploratory Data Analysis",
    "section": "Check variables",
    "text": "Check variables\n\nstr(met)\n\n'data.frame':   2377343 obs. of  30 variables:\n $ USAFID           : int  690150 690150 690150 690150 690150 690150 690150 690150 690150 690150 ...\n $ WBAN             : int  93121 93121 93121 93121 93121 93121 93121 93121 93121 93121 ...\n $ year             : int  2019 2019 2019 2019 2019 2019 2019 2019 2019 2019 ...\n $ month            : int  8 8 8 8 8 8 8 8 8 8 ...\n $ day              : int  1 1 1 1 1 1 1 1 1 1 ...\n $ hour             : int  0 1 2 3 4 5 6 7 8 9 ...\n $ min              : int  56 56 56 56 56 56 56 56 56 56 ...\n $ lat              : num  34.3 34.3 34.3 34.3 34.3 34.3 34.3 34.3 34.3 34.3 ...\n $ lon              : num  -116 -116 -116 -116 -116 ...\n $ elev             : int  696 696 696 696 696 696 696 696 696 696 ...\n $ wind.dir         : int  220 230 230 210 120 NA 320 10 320 350 ...\n $ wind.dir.qc      : chr  \"5\" \"5\" \"5\" \"5\" ...\n $ wind.type.code   : chr  \"N\" \"N\" \"N\" \"N\" ...\n $ wind.sp          : num  5.7 8.2 6.7 5.1 2.1 0 1.5 2.1 2.6 1.5 ...\n $ wind.sp.qc       : chr  \"5\" \"5\" \"5\" \"5\" ...\n $ ceiling.ht       : int  22000 22000 22000 22000 22000 22000 22000 22000 22000 22000 ...\n $ ceiling.ht.qc    : int  5 5 5 5 5 5 5 5 5 5 ...\n $ ceiling.ht.method: chr  \"9\" \"9\" \"9\" \"9\" ...\n $ sky.cond         : chr  \"N\" \"N\" \"N\" \"N\" ...\n $ vis.dist         : int  16093 16093 16093 16093 16093 16093 16093 16093 16093 16093 ...\n $ vis.dist.qc      : chr  \"5\" \"5\" \"5\" \"5\" ...\n $ vis.var          : chr  \"N\" \"N\" \"N\" \"N\" ...\n $ vis.var.qc       : chr  \"5\" \"5\" \"5\" \"5\" ...\n $ temp             : num  37.2 35.6 34.4 33.3 32.8 31.1 29.4 28.9 27.2 26.7 ...\n $ temp.qc          : chr  \"5\" \"5\" \"5\" \"5\" ...\n $ dew.point        : num  10.6 10.6 7.2 5 5 5.6 6.1 6.7 7.8 7.8 ...\n $ dew.point.qc     : chr  \"5\" \"5\" \"5\" \"5\" ...\n $ atm.press        : num  1010 1010 1011 1012 1013 ...\n $ atm.press.qc     : int  5 5 5 5 5 5 5 5 5 5 ...\n $ rh               : num  19.9 21.8 18.5 16.9 17.4 ..."
  },
  {
    "objectID": "slides/week3.html#check-variables-1",
    "href": "slides/week3.html#check-variables-1",
    "title": "Exploratory Data Analysis",
    "section": "Check variables",
    "text": "Check variables\n\nFirst, we see that str() gives us the class of the data, which in this case is a data.frame, as well as the dimensions of the data\nWe also see the variable names and their type (integer, numeric, character, etc.)\nWe can identify major problems with the data at this stage (e.g. a variable that has all missing values)"
  },
  {
    "objectID": "slides/week3.html#check-variables-2",
    "href": "slides/week3.html#check-variables-2",
    "title": "Exploratory Data Analysis",
    "section": "Check variables",
    "text": "Check variables\nWe can get summary statistics on our data.frame using summary().\n\nsummary(met[,8:13])\n\n      lat             lon               elev           wind.dir     \n Min.   :24.55   Min.   :-124.29   Min.   : -13.0   Min.   :  3     \n 1st Qu.:33.97   1st Qu.: -98.02   1st Qu.: 101.0   1st Qu.:120     \n Median :38.35   Median : -91.71   Median : 252.0   Median :180     \n Mean   :37.94   Mean   : -92.15   Mean   : 415.8   Mean   :185     \n 3rd Qu.:41.94   3rd Qu.: -82.99   3rd Qu.: 400.0   3rd Qu.:260     \n Max.   :48.94   Max.   : -68.31   Max.   :9999.0   Max.   :360     \n                                                    NA's   :785290  \n wind.dir.qc        wind.type.code    \n Length:2377343     Length:2377343    \n Class :character   Class :character  \n Mode  :character   Mode  :character"
  },
  {
    "objectID": "slides/week3.html#check-variables-more-closely",
    "href": "slides/week3.html#check-variables-more-closely",
    "title": "Exploratory Data Analysis",
    "section": "Check variables more closely",
    "text": "Check variables more closely\nWe know that we are supposed to have hourly measurements of weather data for the month of August 2019 for the entire US. We should check that we have all of these components. Let’s check:\n\nthe year\nthe month\nthe hours\nthe range of locations (latitude and longitude)"
  },
  {
    "objectID": "slides/week3.html#check-variables-more-closely-1",
    "href": "slides/week3.html#check-variables-more-closely-1",
    "title": "Exploratory Data Analysis",
    "section": "Check variables more closely",
    "text": "Check variables more closely\nWe can generate tables and/or barplots for integer variables:\n\ntable(met$hour)\n\n\n     0      1      2      3      4      5      6      7      8      9     10 \n 99434  93482  93770  96703 110504 112128 106235 101985 100310 102915 101880 \n    11     12     13     14     15     16     17     18     19     20     21 \n100470 103605  97004  96507  97635  94942  94184 100179  94604  94928  96070 \n    22     23 \n 94046  93823 \n\ntable(met$month)\n\n\n      8 \n2377343"
  },
  {
    "objectID": "slides/week3.html#check-variables-more-closely-2",
    "href": "slides/week3.html#check-variables-more-closely-2",
    "title": "Exploratory Data Analysis",
    "section": "Check variables more closely",
    "text": "Check variables more closely\nWe can generate tables and/or barplots for integer variables:\n\nbarplot(table(met$hour))"
  },
  {
    "objectID": "slides/week3.html#check-variables-more-closely-3",
    "href": "slides/week3.html#check-variables-more-closely-3",
    "title": "Exploratory Data Analysis",
    "section": "Check variables more closely",
    "text": "Check variables more closely\nFor numeric variables we should do a summary to see the quantiles, min, max, and mean.\n\ntable(met$year)\n\n\n   2019 \n2377343 \n\nsummary(met$lat)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.55   33.97   38.35   37.94   41.94   48.94 \n\nsummary(met$lon)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-124.29  -98.02  -91.71  -92.15  -82.99  -68.31"
  },
  {
    "objectID": "slides/week3.html#check-variables-more-closely-4",
    "href": "slides/week3.html#check-variables-more-closely-4",
    "title": "Exploratory Data Analysis",
    "section": "Check variables more closely",
    "text": "Check variables more closely\nWe can visualize these distributions with a histogram.\n\nlayout(matrix(1:2, nrow=1))\nhist(met$lat)\nhist(met$lon)\n\n\n\n\n\n\n\nlayout(1)"
  },
  {
    "objectID": "slides/week3.html#check-variables-more-closely-5",
    "href": "slides/week3.html#check-variables-more-closely-5",
    "title": "Exploratory Data Analysis",
    "section": "Check variables more closely",
    "text": "Check variables more closely\nIf we return to our initial question, what weather stations reported the hottest and coldest temperatures, we should take a closer look at our key variable, temperature (temp)\n\nsummary(met$temp)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n -40.00   19.60   23.50   23.59   27.80   56.00   60089 \n\nhist(met$temp)\n\n\nIt looks like the temperatures are in Celsius. A temperature of -40 in August is really cold, we should see if this is an implausible value."
  },
  {
    "objectID": "slides/week3.html#check-variables-more-closely-6",
    "href": "slides/week3.html#check-variables-more-closely-6",
    "title": "Exploratory Data Analysis",
    "section": "Check variables more closely",
    "text": "Check variables more closely\nIt also looks like there is a lot of missing data, encoded by NA values. Let’s check the proportion of missingness by tallying up whether or not every temperature reading is an NA. This will give us a vector of TRUE/FALSE values and then we can take the mean (average), because R automatically interprets TRUE as 1 and FALSE as 0 for mathematical functions.\n\nmean(is.na(met$temp))\n\n[1] 0.0252757\n\n\n2.5% of the data are missing, which is not a huge amount."
  },
  {
    "objectID": "slides/week3.html#validate-against-an-external-source",
    "href": "slides/week3.html#validate-against-an-external-source",
    "title": "Exploratory Data Analysis",
    "section": "Validate against an external source",
    "text": "Validate against an external source\nWe should check outside sources to make sure that our data makes sense. For example the observation with -40C is suspicious, so we should look up the location of the weather station.\nGo to Google maps and enter the coordinates for the site with -40C (29.12, -89.55)\n\nIt doesn’t make much sense to have a -40C reading in the Gulf of Mexico off the coast of Louisiana!"
  },
  {
    "objectID": "slides/week3.html#summary-statistics",
    "href": "slides/week3.html#summary-statistics",
    "title": "Exploratory Data Analysis",
    "section": "Summary statistics",
    "text": "Summary statistics\nThe maximum hourly temperature is 56C at site 720267, and the minimum hourly temperature is -17.2C at site 722817."
  },
  {
    "objectID": "slides/week3.html#summary-statistics-1",
    "href": "slides/week3.html#summary-statistics-1",
    "title": "Exploratory Data Analysis",
    "section": "Summary statistics",
    "text": "Summary statistics\nWe need to transform our data to answer our initial question. Let’s find the daily mean, max, and min temperatures for each weather station in our data.frame. We can do this with the summarize function from the dplyr package. This package is part of the tidyverse, so the syntax is a bit different from what we’ve seen before.\n\nlibrary(dplyr)\nmet_daily &lt;- summarize(met,\n                       temp = mean(temp),\n                       lat = mean(lat),\n                       lon = mean(lon),\n                       elev = mean(elev),\n                       .by = c(USAFID, day))\n\nWhat we’ve done here is told R to summarize the met dataset by the variables USAFID and day, splitting the data into subsets based on those two indexing variables. For each subset (representing a specific station of a specific day), we want the daily average temperature, as well as latitude, longitude, and elevation (though hopefully those don’t change too much over the course of a day!)"
  },
  {
    "objectID": "slides/week3.html#summary-statistics-2",
    "href": "slides/week3.html#summary-statistics-2",
    "title": "Exploratory Data Analysis",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nBefore we continue, check the relative sizes of the met and met_daily objects. Which one is bigger?"
  },
  {
    "objectID": "slides/week3.html#summary-statistics-3",
    "href": "slides/week3.html#summary-statistics-3",
    "title": "Exploratory Data Analysis",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nNow we will order our new dataset by the average daily temperature, just as we ordered the old one by observed temperature.\n\nmet_daily &lt;- met_daily[order(met_daily$temp), ]\n\nhead(met_daily)\n\n    USAFID day       temp    lat    lon elev\n2   722817   3 -17.200000 38.767 -104.3 1838\n1   722817   1 -17.133333 38.767 -104.3 1838\n3   722817   6 -17.066667 38.767 -104.3 1838\n164 726130  11   4.278261 44.270  -71.3 1909\n166 726130  31   4.304348 44.270  -71.3 1909\n163 726130  10   4.583333 44.270  -71.3 1909\n\ntail(met_daily)\n\n      USAFID day     temp      lat       lon     elev\n48708 722749   5 40.85714 33.26900 -111.8120 379.0000\n48695 723805   5 40.97500 34.76800 -114.6180 279.0000\n48721 720339  14 41.00000 32.14600 -111.1710 737.0000\n48710 723805   4 41.18333 34.76800 -114.6180 279.0000\n48688 722787   5 41.35714 33.52700 -112.2950 325.0000\n48438 690150  31 41.71667 34.29967 -116.1657 690.0833"
  },
  {
    "objectID": "slides/week3.html#summary-statistics-4",
    "href": "slides/week3.html#summary-statistics-4",
    "title": "Exploratory Data Analysis",
    "section": "Summary statistics",
    "text": "Summary statistics\nThe maximum daily average temperature is 41.7166667 C at site 690150 and the minimum daily average temperature is -17.2C at site 722817."
  },
  {
    "objectID": "slides/week3.html#summary-statistics-5",
    "href": "slides/week3.html#summary-statistics-5",
    "title": "Exploratory Data Analysis",
    "section": "Summary statistics",
    "text": "Summary statistics\nThe code below is similar to our previous example, but doesn’t include the latitude, longitude, and elevation. How would you alter this code to find the daily median, max, or min temperatures for each station?\n\nsummarize(met,\n          temp = mean(temp),\n          .by = c(USAFID, day))\n\n(try it yourself)"
  },
  {
    "objectID": "slides/week3.html#exploratory-graphs",
    "href": "slides/week3.html#exploratory-graphs",
    "title": "Exploratory Data Analysis",
    "section": "Exploratory graphs",
    "text": "Exploratory graphs\nWith exploratory graphs we aim to:\n\ndebug any issues remaining in the data\nunderstand properties of the data\nlook for patterns in the data\ninform modeling strategies\n\nExploratory graphs do not need to be perfect, we will look at presentation ready plots next week."
  },
  {
    "objectID": "slides/week3.html#exploratory-graphs-1",
    "href": "slides/week3.html#exploratory-graphs-1",
    "title": "Exploratory Data Analysis",
    "section": "Exploratory graphs",
    "text": "Exploratory graphs\nExamples of exploratory graphs include:\n\nhistograms\nboxplots\nscatterplots\nsimple maps"
  },
  {
    "objectID": "slides/week3.html#exploratory-graphs-2",
    "href": "slides/week3.html#exploratory-graphs-2",
    "title": "Exploratory Data Analysis",
    "section": "Exploratory Graphs",
    "text": "Exploratory Graphs\nFocusing on the variable of interest, temperature, let’s look at the distribution (after removing -40C)\n\nhist(met$temp)"
  },
  {
    "objectID": "slides/week3.html#exploratory-graphs-3",
    "href": "slides/week3.html#exploratory-graphs-3",
    "title": "Exploratory Data Analysis",
    "section": "Exploratory Graphs",
    "text": "Exploratory Graphs\nLet’s look at the daily data\n\nhist(met_daily$temp)"
  },
  {
    "objectID": "slides/week3.html#exploratory-graphs-4",
    "href": "slides/week3.html#exploratory-graphs-4",
    "title": "Exploratory Data Analysis",
    "section": "Exploratory Graphs",
    "text": "Exploratory Graphs\nA boxplot gives us an idea of the quantiles of the distribution and any outliers\n\nboxplot(met$temp, col = \"blue\")"
  },
  {
    "objectID": "slides/week3.html#exploratory-graphs-5",
    "href": "slides/week3.html#exploratory-graphs-5",
    "title": "Exploratory Data Analysis",
    "section": "Exploratory Graphs",
    "text": "Exploratory Graphs\nLet’s look at the daily data\n\nboxplot(met_daily$temp, col = \"blue\")"
  },
  {
    "objectID": "slides/week3.html#exploratory-graphs-6",
    "href": "slides/week3.html#exploratory-graphs-6",
    "title": "Exploratory Data Analysis",
    "section": "Exploratory Graphs",
    "text": "Exploratory Graphs\nWe know that these data come from US weather stations, so we might have some idea what to expect just from plotting the latitude and longitude (note that we fix the aspect ratio at 1:1 with asp = 1; this prevents the plot from stretching or shrinking to fit the available plotting area):\n\nplot(met_daily$lon, met_daily$lat, asp=1)"
  },
  {
    "objectID": "slides/week3.html#exploratory-graphs-7",
    "href": "slides/week3.html#exploratory-graphs-7",
    "title": "Exploratory Data Analysis",
    "section": "Exploratory Graphs",
    "text": "Exploratory Graphs\nA map will show us where the weather stations are located. First let’s get the unique latitudes and longitudes and see how many meteorological sites there are\n\nmet_stations &lt;- (unique(met[,c(\"lat\",\"lon\")]))  \ndim(met_stations)\n\n[1] 2827    2"
  },
  {
    "objectID": "slides/week3.html#exploratory-graphs-8",
    "href": "slides/week3.html#exploratory-graphs-8",
    "title": "Exploratory Data Analysis",
    "section": "Exploratory Graphs",
    "text": "Exploratory Graphs\nA map will show us where the weather stations are located. First let’s get the unique latitudes and longitudes and see how many meteorological sites there are.\n\nlibrary(leaflet)\nleaflet(met_stations) |&gt; \n  addProviderTiles('CartoDB.Positron') |&gt; \n  addCircles(lat = ~lat, lng = ~lon,\n             opacity = 1, fillOpacity = 1, radius = 400)"
  },
  {
    "objectID": "slides/week3.html#exploratory-graphs-9",
    "href": "slides/week3.html#exploratory-graphs-9",
    "title": "Exploratory Data Analysis",
    "section": "Exploratory Graphs",
    "text": "Exploratory Graphs"
  },
  {
    "objectID": "slides/week3.html#exploratory-graphs-10",
    "href": "slides/week3.html#exploratory-graphs-10",
    "title": "Exploratory Data Analysis",
    "section": "Exploratory Graphs",
    "text": "Exploratory Graphs\nLet’s map the locations of the max and min daily temperatures.\n\nmin &lt;- met_daily[1, ]               # First observation\nmax &lt;- met_daily[nrow(met_daily), ] # Last observation\n\nleaflet() |&gt; \n  addProviderTiles('CartoDB.Positron') |&gt; \n  addCircles(\n    data = min,\n    lat = ~lat, lng = ~lon, popup = \"Min temp.\",\n    opacity = 1, fillOpacity = 1, radius = 400, color = \"blue\"\n    ) |&gt;\n  addCircles(\n    data = max,\n    lat = ~lat, lng = ~lon, popup = \"Max temp.\",\n    opacity=1, fillOpacity=1, radius = 400, color = \"red\"\n    )\n\n(next slide)"
  },
  {
    "objectID": "slides/week3.html#exploratory-data-analysis-1",
    "href": "slides/week3.html#exploratory-data-analysis-1",
    "title": "Exploratory Data Analysis",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nSince our eyes and brains are not wired to detect patterns in large data tables filled with text and numbers, communication about data […] rarely comes in the form of raw data or code output. Instead, data and data-driven results are usually either summarized (e.g., using an average/mean) and presented in small summary tables or they are presented visually in the form of graphs, in which shape, distance, color, and size can be used to represent the magnitudes of (and relationships between) the values within our data.\n\n\nViridical Data Science, Yu and Barter"
  },
  {
    "objectID": "slides/week3.html#working-with-data.frames",
    "href": "slides/week3.html#working-with-data.frames",
    "title": "Exploratory Data Analysis",
    "section": "Working with data.frames",
    "text": "Working with data.frames\nThis gave as a data.frame object, which is a standard R format for cleaned, rectangular data. Each row represents an observation and each column represents a variable.\nAs we have seen, you can access particular parts of the data.frame by subsetting with the square brackets, [,]. For example, you can pull out the 2nd, 3rd, and 4th elements of the 1st column of our met dataset with met[2:4, 1].\nYou can also pull out specific columns by name, using the $ operator. Since the first column is called USAFID, we could access the same subset as above with met$USAFID[2:4] (notice that there is no comma here, because we have already subset down to a single variable).\nTo see the list of names for the dataset, you can use names(met) or colnames(met). To see the top few rows of the dataset, use head(met)."
  },
  {
    "objectID": "slides/week3.html#sorting",
    "href": "slides/week3.html#sorting",
    "title": "Exploratory Data Analysis",
    "section": "Sorting",
    "text": "Sorting\nIf we return to our initial question (“Which weather stations reported the hottest and coldest daily temperatures?”), we need to generate a list of weather stations that are ordered from highest to lowest. We can then examine the top and bottom of this new dataset.\nFirst let us remove the aberrant observations and then we’ll sort by temperature.\n\nmet &lt;- met[met$temp &gt; -40, ]\n\nNotice that we do not create a new object, we just overwrite the met object. Once you’re sure that you want to remove certain observations, this is a good way to avoid confusion (otherwise, it is easy to end up with multiple subsets of the data in your R environment with similar names like met, met_ss, met_ss2, met_final, met_FINAL, met_FINAL_REAL, etc.)"
  },
  {
    "objectID": "slides/week3.html#highest-and-lowest",
    "href": "slides/week3.html#highest-and-lowest",
    "title": "Exploratory Data Analysis",
    "section": "Highest and Lowest",
    "text": "Highest and Lowest\n\nhead(met)[,c(1,8:10,24)]\n\n        USAFID    lat    lon elev  temp\n1203053 722817 38.767 -104.3 1838 -17.2\n1203055 722817 38.767 -104.3 1838 -17.2\n1203128 722817 38.767 -104.3 1838 -17.2\n1203129 722817 38.767 -104.3 1838 -17.2\n1203222 722817 38.767 -104.3 1838 -17.2\n1203225 722817 38.767 -104.3 1838 -17.2\n\ntail(met)[,c(1,8:10,24)]\n\n      USAFID    lat      lon elev temp\n42783 720267 38.955 -121.081  467 52.0\n724   690150 34.300 -116.166  696 52.8\n749   690150 34.296 -116.162  625 52.8\n748   690150 34.300 -116.166  696 53.9\n701   690150 34.300 -116.166  696 54.4\n42403 720267 38.955 -121.081  467 56.0"
  },
  {
    "objectID": "slides/week1.html#r-stuff",
    "href": "slides/week1.html#r-stuff",
    "title": "Welcome!",
    "section": "r stuff",
    "text": "r stuff\n\nsummary(1:10)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    3.25    5.50    5.50    7.75   10.00"
  },
  {
    "objectID": "slides/week3.html#the-tidyverse-model",
    "href": "slides/week3.html#the-tidyverse-model",
    "title": "Exploratory Data Analysis",
    "section": "The Tidyverse Model",
    "text": "The Tidyverse Model\n\nLoosely, EDA encompasses the Import -&gt; Tidy -&gt; Transform -&gt; Visualize steps. Basically it is everything before we do modeling, prediction or inference.\nEDA may involve some statistical summaries, but it does not include formal statistical analysis."
  },
  {
    "objectID": "slides/week3.html#check-variables-more-closely-7",
    "href": "slides/week3.html#check-variables-more-closely-7",
    "title": "Exploratory Data Analysis",
    "section": "Check variables more closely",
    "text": "Check variables more closely\nIn our data.frame we can easily subset the data and select certain columns. Here, we select all observations with a temperature of -40C and a specific subset of the variables:\n\nmet_ss &lt;- met[met$temp == -40.00, c('hour','lat','lon','elev','wind.sp')]\n\ndim(met_ss)\n\n[1] 60125     5\n\nsummary(met_ss)\n\n      hour            lat             lon              elev      \n Min.   : 0.00   Min.   :29.12   Min.   :-89.55   Min.   :36     \n 1st Qu.: 2.75   1st Qu.:29.12   1st Qu.:-89.55   1st Qu.:36     \n Median : 5.50   Median :29.12   Median :-89.55   Median :36     \n Mean   : 5.50   Mean   :29.12   Mean   :-89.55   Mean   :36     \n 3rd Qu.: 8.25   3rd Qu.:29.12   3rd Qu.:-89.55   3rd Qu.:36     \n Max.   :11.00   Max.   :29.12   Max.   :-89.55   Max.   :36     \n NA's   :60089   NA's   :60089   NA's   :60089    NA's   :60089  \n    wind.sp     \n Min.   : NA    \n 1st Qu.: NA    \n Median : NA    \n Mean   :NaN    \n 3rd Qu.: NA    \n Max.   : NA    \n NA's   :60125"
  },
  {
    "objectID": "slides/week3.html#check-variables-more-closely-8",
    "href": "slides/week3.html#check-variables-more-closely-8",
    "title": "Exploratory Data Analysis",
    "section": "Check variables more closely",
    "text": "Check variables more closely\nIn dplyr we can do the same thing using filter and select\n\nlibrary(dplyr)\nmet_ss &lt;- filter(met, temp == -40.00) |&gt; \n  select(USAFID, day, hour, lat, lon, elev, wind.sp)\n\ndim(met_ss)\n\n[1] 36  7\n\nsummary(met_ss)\n\n     USAFID            day         hour            lat             lon        \n Min.   :720717   Min.   :1   Min.   : 0.00   Min.   :29.12   Min.   :-89.55  \n 1st Qu.:720717   1st Qu.:1   1st Qu.: 2.75   1st Qu.:29.12   1st Qu.:-89.55  \n Median :720717   Median :1   Median : 5.50   Median :29.12   Median :-89.55  \n Mean   :720717   Mean   :1   Mean   : 5.50   Mean   :29.12   Mean   :-89.55  \n 3rd Qu.:720717   3rd Qu.:1   3rd Qu.: 8.25   3rd Qu.:29.12   3rd Qu.:-89.55  \n Max.   :720717   Max.   :1   Max.   :11.00   Max.   :29.12   Max.   :-89.55  \n                                                                              \n      elev       wind.sp   \n Min.   :36   Min.   : NA  \n 1st Qu.:36   1st Qu.: NA  \n Median :36   Median : NA  \n Mean   :36   Mean   :NaN  \n 3rd Qu.:36   3rd Qu.: NA  \n Max.   :36   Max.   : NA  \n              NA's   :36"
  },
  {
    "objectID": "slides/week3.html#data-cleaning",
    "href": "slides/week3.html#data-cleaning",
    "title": "Exploratory Data Analysis",
    "section": "Data cleaning",
    "text": "Data cleaning\nIf we return to our initial question (“Which weather stations reported the hottest and coldest daily temperatures?”), we need to generate a list of weather stations that are ordered from highest to lowest. We can then examine the top and bottom of this new dataset.\nFirst let us remove the aberrant observations and then we’ll sort by temperature.\n\nmet &lt;- met[met$temp &gt; -40, ]\n\nNotice that we do not create a new object, we just overwrite the met object. Once you’re sure that you want to remove certain observations, this is a good way to avoid confusion (otherwise, it is easy to end up with multiple subsets of the data in your R environment with similar names like met, met_ss, met_ss2, met_final, met_FINAL, met_FINAL_REAL, etc.)"
  },
  {
    "objectID": "slides/week3.html#data-cleaning-1",
    "href": "slides/week3.html#data-cleaning-1",
    "title": "Exploratory Data Analysis",
    "section": "Data cleaning",
    "text": "Data cleaning\nWe will also remove any observations with missing temperature values (NA).\nThe is.na() function tells you whether or not a particular value is missing and the ! operator takes the opposite of a TRUE/FALSE value, so in combination, they tell you which observations are not missing.\n\nmet &lt;- met[!is.na(met$temp), ]"
  },
  {
    "objectID": "slides/week3.html#exploratory-graphs-11",
    "href": "slides/week3.html#exploratory-graphs-11",
    "title": "Exploratory Data Analysis",
    "section": "Exploratory Graphs",
    "text": "Exploratory Graphs\nScatterplots help us look at pairwise relationships. Let’s see if there is any trend in temperature with latitude\n\nplot(met_daily$lat, met_daily$temp, pch=16, cex=0.5)\n\n\n\n\n\n\n\n\nThere is a clear decrease in temperatures as you increase in latitude (i.e as you go north)."
  },
  {
    "objectID": "slides/week3.html#exploratory-graphs-12",
    "href": "slides/week3.html#exploratory-graphs-12",
    "title": "Exploratory Data Analysis",
    "section": "Exploratory Graphs",
    "text": "Exploratory Graphs\nWe can add a simple linear regression line to this plot using lm() and abline(). We can also add a title and change the axis labels.\n\nmod &lt;- lm(temp ~ lat, data = met_daily)\nmet_daily[, plot(\n  lat, temp, pch=19, cex=0.5, \n  main = \"Temperature and Latitude\", \n  xlab = \"Latitude\", ylab = \"Temperature (deg C)\")\n  ]\nabline(mod, lwd=2, col=\"red\")\n\n(next slide)"
  }
]